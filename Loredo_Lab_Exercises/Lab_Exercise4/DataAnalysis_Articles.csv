"","Title","Author","Subject","Abstract","Meta"
"1","Testing the $\mathbfΛ$CDM Cosmological Model with Forthcoming Measurements of the Cosmic Microwave Background with SPT-3G","K. Prabhu, S. Raghunathan, M. Millea, G. Lynch, P. A. R. Ade, E. Anderes, A. J. Anderson, B. Ansarinejad, M. Archipley, L. Balkenhol, K. Benabed, A. N. Bender, B. A. Benson, F. Bianchini, L. E. Bleem, F. R. Bouchet, L. Bryant, E. Camphuis, J. E. Carlstrom, T. W. Cecil, C. L. Chang, P. Chaubal, P. M. Chichura, T.-L. Chou, A. Coerver, T. M. Crawford, A. Cukierman, C. Daley, T. de Haan, K. R. Dibert, M. A. Dobbs, A. Doussot, D. Dutcher, W. Everett, C. Feng, K. R. Ferguson, K. Fichman, A. Foster, S. Galli, A. E. Gambrel, R. W. Gardner, F. Ge, N. Goeckner-Wald, R. Gualtieri, F. Guidi, S. Guns, N. W. Halverson, E. Hivon, G. P. Holder, W. L. Holzapfel, J. C. Hood, A. Hryciuk, N. Huang, F. Kéruzoré, L. Knox, M. Korman, K. Kornoelje, C.-L. Kuo, A. T. Lee, K. Levy, A. E. Lowitz, C. Lu, A. Maniyar, F. Menanteau, J. Montgomery, Y. Nakato, T. Natoli, G. I. Noble, V. Novosad, Y. Omori, S. Padin, Z. Pan, P. Paschos, K. A. Phadke, W. Quan, M. Rahimi, A. Rahlin, C. L. Reichardt, M. Rouble, J. E. Ruhl, E. Schiappucci, G. Smecher, J. A. Sobrin, A. A. Stark, J. Stephen, A. Suzuki, C. Tandoi, K. L. Thompson, B. Thorne, C. Trendafilova, C. Tucker, C. Umilta, A. Vitrier, J. D. Vieira, Y. Wan, G. Wang, N. Whitehorn, W. L. K. Wu, V. Yefremenko, M. R. Young, J. A. Zebrowski","Cosmology and Nongalactic Astrophysics (astro-ph.CO)","We forecast constraints on cosmological parameters enabled by three surveys conducted with SPT-3G, the third-generation camera on the South Pole Telescope. The surveys cover separate regions of 1500, 2650, and 6000 ${\rm deg}^{2}$ to different depths, in total observing 25% of the sky. These regions will be measured to white noise levels of roughly 2.5, 9, and 12 $\mu{\rm K-arcmin}$, respectively, in CMB temperature units at 150 GHz by the end of 2024. The survey also includes measurements at 95 and 220 GHz, which have noise levels a factor of ~1.2 and 3.5 times higher than 150 GHz, respectively, with each band having a polarization noise level ~$\sqrt{\text{2}}$ times higher than the temperature noise. We use a novel approach to obtain the covariance matrices for jointly and optimally estimated gravitational lensing potential bandpowers and unlensed CMB temperature and polarization bandpowers. We demonstrate the ability to test the $\Lambda{\rm CDM}$ model via the consistency of cosmological parameters constrained independently from SPT-3G and Planck data, and consider the improvement in constraints on $\Lambda{\rm CDM}$ extension parameters from a joint analysis of SPT-3G and Planck data. The $\Lambda{\rm CDM}$ cosmological parameters are typically constrained with uncertainties up to ~2 times smaller with SPT-3G data, compared to Planck, with the two data sets measuring significantly different angular scales and polarization levels, providing additional tests of the standard cosmological model.","Tue, 26 Mar 2024 17:57:20 UTC (4,338 KB)"
"2","Empowering Data Mesh with Federated Learning","Haoyuan Li, Salman Toor","Machine Learning (cs.LG)","The evolution of data architecture has seen the rise of data lakes, aiming to solve the bottlenecks of data management and promote intelligent decision-making. However, this centralized architecture is limited by the proliferation of data sources and the growing demand for timely analysis and processing. A new data paradigm, Data Mesh, is proposed to overcome these challenges. Data Mesh treats domains as a first-class concern by distributing the data ownership from the central team to each data domain, while keeping the federated governance to monitor domains and their data products. Many multi-million dollar organizations like Paypal, Netflix, and Zalando have already transformed their data analysis pipelines based on this new architecture. In this decentralized architecture where data is locally preserved by each domain team, traditional centralized machine learning is incapable of conducting effective analysis across multiple domains, especially for security-sensitive organizations. To this end, we introduce a pioneering approach that incorporates Federated Learning into Data Mesh. To the best of our knowledge, this is the first open-source applied work that represents a critical advancement toward the integration of federated learning methods into the Data Mesh paradigm, underscoring the promising prospects for privacy-preserving and decentralized data analysis strategies within Data Mesh architecture.","Tue, 26 Mar 2024 17:10:15 UTC (929 KB)"
"3","To Supervise or Not to Supervise: Understanding and Addressing the Key Challenges of 3D Transfer Learning","Souhail Hadgi, Lei Li, Maks Ovsjanikov","Computer Vision and Pattern Recognition (cs.CV)","Transfer learning has long been a key factor in the advancement of many fields including 2D image analysis. Unfortunately, its applicability in 3D data processing has been relatively limited. While several approaches for 3D transfer learning have been proposed in recent literature, with contrastive learning gaining particular prominence, most existing methods in this domain have only been studied and evaluated in limited scenarios. Most importantly, there is currently a lack of principled understanding of both when and why 3D transfer learning methods are applicable. Remarkably, even the applicability of standard supervised pre-training is poorly understood. In this work, we conduct the first in-depth quantitative and qualitative investigation of supervised and contrastive pre-training strategies and their utility in downstream 3D tasks. We demonstrate that layer-wise analysis of learned features provides significant insight into the downstream utility of trained networks. Informed by this analysis, we propose a simple geometric regularization strategy, which improves the transferability of supervised pre-training. Our work thus sheds light onto both the specific challenges of 3D transfer learning, as well as strategies to overcome them.","Tue, 26 Mar 2024 16:57:33 UTC (5,797 KB)"
"4","Using Domain Knowledge to Guide Dialog Structure Induction via Neural Probabilistic Soft Logic","Connor Pryor, Quan Yuan, Jeremiah Liu, Mehran Kazemi, Deepak Ramachandran, Tania Bedrax-Weiss, Lise Getoor","Computation and Language (cs.CL)","Dialog Structure Induction (DSI) is the task of inferring the latent dialog structure (i.e., a set of dialog states and their temporal transitions) of a given goal-oriented dialog. It is a critical component for modern dialog system design and discourse analysis. Existing DSI approaches are often purely data-driven, deploy models that infer latent states without access to domain knowledge, underperform when the training corpus is limited/noisy, or have difficulty when test dialogs exhibit distributional shifts from the training domain. This work explores a neural-symbolic approach as a potential solution to these problems. We introduce Neural Probabilistic Soft Logic Dialogue Structure Induction (NEUPSL DSI), a principled approach that injects symbolic knowledge into the latent space of a generative neural model. We conduct a thorough empirical investigation on the effect of NEUPSL DSI learning on hidden representation quality, few-shot learning, and out-of-domain generalization performance. Over three dialog structure induction datasets and across unsupervised and semi-supervised settings for standard and cross-domain generalization, the injection of symbolic knowledge using NEUPSL DSI provides a consistent boost in performance over the canonical baselines.","Tue, 26 Mar 2024 16:42:30 UTC (3,380 KB)"
"5","Learning the Optimal Power Flow: Environment Design Matters","Thomas Wolgast, Astrid Nieße","Machine Learning (cs.LG)","To solve the optimal power flow (OPF) problem, reinforcement learning (RL) emerges as a promising new approach. However, the RL-OPF literature is strongly divided regarding the exact formulation of the OPF problem as an RL environment. In this work, we collect and implement diverse environment design decisions from the literature regarding training data, observation space, episode definition, and reward function choice. In an experimental analysis, we show the significant impact of these environment design options on RL-OPF training performance. Further, we derive some first recommendations regarding the choice of these design decisions. The created environment framework is fully open-source and can serve as a benchmark for future research in the RL-OPF field.","Tue, 26 Mar 2024 16:13:55 UTC (177 KB)"
"6","Band Structure and Fermi Surface Nesting in $LaSb_2$","Evan O'Leary, Lin-Lin Wang, Yevhen Kushnirenko, Ben Schrunk, Andrew Eaton, Paula Herrera-Siklody, Paul C. Canfield, Adam Kaminski","Strongly Correlated Electrons (cond-mat.str-el)","We use high-resolution angle resolved photoemission spectroscopy (ARPES) and density functional theory (DFT) to investigate the electronic structure of the charge density wave (CDW) system LaSb$_2$. This compound is among an interesting group of materials that manifests both a CDW transition and lower temperature superconductivity. We find the DFT calculations to be in good agreement with our ARPES data. The Fermi surface of LaSb$_2$ consists of two small hole pockets close to $\Gamma$ and four larger pockets near the Brillouin zone (BZ) boundary. The overall features of the Fermi surface do not vary with temperature. A saddle point is present at -0.19 $eV$ below the Fermi level at $\Gamma$. Critical points in band structure have more pronounced effects on a materials properties when they are located closer to the Fermi level, making doped LaSb$_2$ compounds a potential interesting subject of future research. Multiple peaks are present in the generalized, electronic susceptibility calculations indicating the presence of possible nesting vectors. We were not able to detect any signatures of the CDW transition at 355 K, pointing to the subtle nature of this transition. This is unusual, given that such a high transition temperature is expected to be associated with the presence of a large CDW gap. This is confirmed through investigation of the Fermi surface and through analysis of momentum distribution curves (MDC). It is possible that changes are subtle and occur below current sensitivity of our measurements.","Tue, 26 Mar 2024 16:06:07 UTC (9,603 KB)"
"7","Towards Multilevel Modelling of Train Passing Events on the Staffordshire Bridge","Lawrence A. Bull, Chiho Jeon, Mark Girolami, Andrew Duncan, Jennifer Schooling, Miguel Bravo Haro","Applications (stat.AP)","We suggest a multilevel model, to represent aggregate train-passing events from the Staffordshire bridge monitoring system. We formulate a combined model from simple units, representing strain envelopes (of each train passing) for two types of commuter train. The measurements are treated as a longitudinal dataset and represented with a (low-rank approximation) hierarchical Gaussian process. For each unit in the combined model, we encode domain expertise as boundary condition constraints and work towards a general representation of the strain response. Looking forward, this should allow for the simulation of train types that were previously unobserved in the training data. For example, trains with more passengers or freights with a heavier payload. The strain event simulations are valuable since they can inform further experiments (including FEM calibration, fatigue analysis, or design) to test the bridge in hypothesised scenarios.","Tue, 26 Mar 2024 15:55:54 UTC (4,823 KB)"
"8","CSSTs: A Dynamic Data Structure for Partial Orders in Concurrent Execution Analysis","Hünkar Can Tunç, Ameya Prashant Deshmukh, Berk Çirisci, Constantin Enea, Andreas Pavlogiannis","Programming Languages (cs.PL)","Dynamic analyses are a standard approach to analyzing and testing concurrent programs. Such techniques observe program traces and analyze them to infer the presence or absence of bugs. At its core, each analysis maintains a partial order $P$ that represents order dependencies between events of the analyzed trace $\sigma$. Naturally, the scalability of the analysis largely depends on how efficiently it maintains $P$. The standard data structure for this task has thus far been vector clocks. These, however, are slow for analyses that follow a non-streaming style, costing $O(n)$ for inserting (and propagating) each new ordering in $P$, where $n$ is the size of $\sigma$, while they cannot handle the deletion of existing orderings.
In this paper we develop collective sparse segment trees (CSSTs), a simple but elegant data structure for generically maintaining a partial order $P$. CSSTs thrive when the width $k$ of $P$ is much smaller than the size $n$ of its domain, allowing inserting, deleting, and querying for orderings in $P$ to run in $O(logn)$ time. For a concurrent trace, $k$ is bounded by the number of its threads, and is normally orders of magnitude smaller than its size $n$, making CSSTs fitting for this setting. Our experimental results confirm that CSSTs are the best data structure currently to handle a range of dynamic analyses from existing literature.","Tue, 26 Mar 2024 15:53:53 UTC (124 KB)"
"9","Probing the shape of the quark-gluon plasma droplet via event-by-event QGP tomography","Bithika Karmakar, Dusan Zigic, Pasi Huovinen, Marko Djordjevic, Magdalena Djordjevic, Jussi Auvinen","High Energy Physics - Phenomenology (hep-ph)","This study investigates Quark-Gluon Plasma (QGP) in heavy-ion collisions through two avenues: high-$p_{\perp}$ frameworks and hydrodynamic modeling. Using the T$_{\text{R}}$ENTo model, we find that IP-Glasma mimicking $p=0$ value aligns well with high-$p_{\perp}$ data, in agreement with Bayesian analysis of the low-$p_{\perp}$ regime. While adjusting $p$ values may improve a fit to a particular high-$p_{\perp}$ observable, it does not permit an earlier onset of transverse expansion.","Tue, 26 Mar 2024 15:53:07 UTC (884 KB)"
"10","Graph Language Model (GLM): A new graph-based approach to detect social instabilities","Wallyson Lemes de Oliveira, Vahid Shamsaddini, Ali Ghofrani, Rahul Singh Inda, Jithendra Sai Veeramaneni, Étienne Voutaz","Computation and Language (cs.CL)","This scientific report presents a novel methodology for the early prediction of important political events using News datasets. The methodology leverages natural language processing, graph theory, clique analysis, and semantic relationships to uncover hidden predictive signals within the data. Initially, we designed a preliminary version of the method and tested it on a few events. This analysis revealed limitations in the initial research phase. We then enhanced the model in two key ways: first, we added a filtration step to only consider politically relevant news before further processing; second, we adjusted the input features to make the alert system more sensitive to significant spikes in the data. After finalizing the improved methodology, we tested it on eleven events including US protests, the Ukraine war, and French protests. Results demonstrate the superiority of our approach compared to baseline methods. Through targeted refinements, our model can now provide earlier and more accurate predictions of major political events based on subtle patterns in news data.","Tue, 26 Mar 2024 15:53:02 UTC (15,022 KB)"
"11","Exploring the Boundaries of Ambient Awareness in Twitter","Pablo Sanchez-Martin, Sonja Utz, Isabel Valera","Social and Information Networks (cs.SI)","Ambient awareness refers to the ability of social media users to obtain knowledge about who knows what (i.e., users' expertise) in their network, by simply being exposed to other users' content (e.g, tweets on Twitter). Previous work, based on user surveys, reveals that individuals self-report ambient awareness only for parts of their networks. However, it is unclear whether it is their limited cognitive capacity or the limited exposure to diagnostic tweets (i.e., online content) that prevents people from developing ambient awareness for their complete network. In this work, we focus on in-wall ambient awareness (IWAA) in Twitter and conduct a two-step data-driven analysis, that allows us to explore to which extent IWAA is likely, or even possible. First, we rely on reactions (e.g., likes), as strong evidence of users being aware of experts in Twitter. Unfortunately, such strong evidence can be only measured for active users, which represent the minority in the network. Thus to study the boundaries of IWAA to a larger extent, in the second part of our analysis, we instead focus on the passive exposure to content generated by other users -- which we refer to as in-wall visibility. This analysis shows that (in line with \citet{levordashka2016ambient}) only for a subset of users IWAA is plausible, while for the majority it is unlikely, if even possible, to develop IWAA. We hope that our methodology paves the way for the emergence of data-driven approaches for the study of ambient awareness.","Tue, 26 Mar 2024 15:09:33 UTC (2,405 KB)"
"12","Noise2Noise Denoising of CRISM Hyperspectral Data","Robert Platt, Rossella Arcucci, Cédric M. John","Computer Vision and Pattern Recognition (cs.CV)","Hyperspectral data acquired by the Compact Reconnaissance Imaging Spectrometer for Mars (CRISM) have allowed for unparalleled mapping of the surface mineralogy of Mars. Due to sensor degradation over time, a significant portion of the recently acquired data is considered unusable. Here a new data-driven model architecture, Noise2Noise4Mars (N2N4M), is introduced to remove noise from CRISM images. Our model is self-supervised and does not require zero-noise target data, making it well suited for use in Planetary Science applications where high quality labelled data is scarce. We demonstrate its strong performance on synthetic-noise data and CRISM images, and its impact on downstream classification performance, outperforming benchmark methods on most metrics. This allows for detailed analysis for critical sites of interest on the Martian surface, including proposed lander sites.","Tue, 26 Mar 2024 14:49:22 UTC (3,683 KB)"
"13","Regularity for nonlocal equations with local Neumann boundary conditions","Xavier Ros-Oton, Marvin Weidner","Analysis of PDEs (math.AP)","In this article we establish fine results on the boundary behavior of solutions to nonlocal equations in $C^{k,\gamma}$ domains which satisfy local Neumann conditions on the boundary. Such solutions typically blow up at the boundary like $v \asymp d^{s-1}$ and are sometimes called large solutions. In this setup we prove optimal regularity results for the quotients $v/d^{s-1}$, depending on the regularity of the domain and on the data of the problem. The results of this article will be important in a forthcoming work on nonlocal free boundary problems.","Tue, 26 Mar 2024 14:11:19 UTC (46 KB)"
"14","Nuclear matrix elements of neutrinoless double-beta decay in covariant density functional theory with different mechanisms","C. R. Ding, Gang Li, J. M. Yao","Nuclear Theory (nucl-th)","Nuclear matrix elements (NMEs) for neutrinoless double-beta ($0\nu\beta\beta$) decay in candidate nuclei play a crucial role in interpreting results from current experiments and in designing future ones. Accurate NME values serve as important nuclear inputs for constraining parameters in new physics, such as neutrino mass and the Wilson coefficients of lepton-number-violating (LNV) operators. In this study, we present a comprehensive calculation of NMEs for $0\nu\beta\beta$ decay in $^{76}$Ge, $^{82}$Se, $^{100}$Mo, $^{130}$Te, and $^{136}$Xe, using nuclear wave functions obtained from multi-reference covariant density functional theory (MR-CDFT). We employ three types of transition potentials at the leading order in chiral effective field theory. Our results, along with recent data, are utilized to constrain the coefficients of LNV operators. We find that NMEs for the standard and short-range mechanisms are significantly larger than those for the non-standard long-range mechanism. The use of NMEs from various nuclear models does not notably change the parameter space intervals for the coefficients, although MR-CDFT yields the most stringent constraint. Furthermore, our NMEs can also be used to perform a more comprehensive analysis with multiple isotopes.","Tue, 26 Mar 2024 14:06:23 UTC (2,985 KB)"
"15","Low-temperature benchmarking of qubit control wires by primary electron thermometry","Elias Roos Hansen, Ferdinand Kuemmeth, Joost van der Heijden","Mesoscale and Nanoscale Physics (cond-mat.mes-hall)","Low-frequency qubit control wires require non-trivial thermal anchoring and low-pass filtering. The resulting electron temperature serves as a quality benchmark for these signal lines. In this technical note, we make use of a primary electron thermometry technique, using a Coulomb blockade thermometer, to establish the electron temperature in the millikelvin regime. The experimental four-probe measurement setup, the data analysis, and the measurement limitations are discussed in detail. We verify the results by also using another electron thermometry technique, based on a superconductor-insulator-normal metal junction. Our comparison of signal lines with QDevil's QFilter to unfiltered signal lines demonstrates that the filter significantly reduces both the rms noise and electron temperature, which is measured to be 22 $\pm$ 1 mK.","Tue, 26 Mar 2024 14:03:19 UTC (8,966 KB)"
"16","Panonut360: A Head and Eye Tracking Dataset for Panoramic Video","Yutong Xu, Junhao Du, Jiahe Wang, Yuwei Ning, Sihan Zhou Yang Cao","Computer Vision and Pattern Recognition (cs.CV)","With the rapid development and widespread application of VR/AR technology, maximizing the quality of immersive panoramic video services that match users' personal preferences and habits has become a long-standing challenge. Understanding the saliency region where users focus, based on data collected with HMDs, can promote multimedia encoding, transmission, and quality assessment. At the same time, large-scale datasets are essential for researchers and developers to explore short/long-term user behavior patterns and train AI models related to panoramic videos. However, existing panoramic video datasets often include low-frequency user head or eye movement data through short-term videos only, lacking sufficient data for analyzing users' Field of View (FoV) and generating video saliency regions.
Driven by these practical factors, in this paper, we present a head and eye tracking dataset involving 50 users (25 males and 25 females) watching 15 panoramic videos. The dataset provides details on the viewport and gaze attention locations of users. Besides, we present some statistics samples extracted from the dataset. For example, the deviation between head and eye movements challenges the widely held assumption that gaze attention decreases from the center of the FoV following a Gaussian distribution. Our analysis reveals a consistent downward offset in gaze fixations relative to the FoV in experimental settings involving multiple users and videos. That's why we name the dataset Panonut, a saliency weighting shaped like a donut. Finally, we also provide a script that generates saliency distributions based on given head or eye coordinates and pre-generated saliency distribution map sets of each video from the collected eye tracking data.
The dataset is available on website: this https URL.","Tue, 26 Mar 2024 13:54:52 UTC (11,373 KB)"
"17","Not All Similarities Are Created Equal: Leveraging Data-Driven Biases to Inform GenAI Copyright Disputes","Uri Hacohen, Adi Haviv, Shahar Sarfaty, Bruria Friedman, Niva Elkin-Koren, Roi Livni, Amit H Bermano","Computer Vision and Pattern Recognition (cs.CV)","The advent of Generative Artificial Intelligence (GenAI) models, including GitHub Copilot, OpenAI GPT, and Stable Diffusion, has revolutionized content creation, enabling non-professionals to produce high-quality content across various domains. This transformative technology has led to a surge of synthetic content and sparked legal disputes over copyright infringement. To address these challenges, this paper introduces a novel approach that leverages the learning capacity of GenAI models for copyright legal analysis, demonstrated with GPT2 and Stable Diffusion models. Copyright law distinguishes between original expressions and generic ones (Scènes à faire), protecting the former and permitting reproduction of the latter. However, this distinction has historically been challenging to make consistently, leading to over-protection of copyrighted works. GenAI offers an unprecedented opportunity to enhance this legal analysis by revealing shared patterns in preexisting works. We propose a data-driven approach to identify the genericity of works created by GenAI, employing ""data-driven bias"" to assess the genericity of expressive compositions. This approach aids in copyright scope determination by utilizing the capabilities of GenAI to identify and prioritize expressive elements and rank them according to their frequency in the model's dataset. The potential implications of measuring expressive genericity for copyright law are profound. Such scoring could assist courts in determining copyright scope during litigation, inform the registration practices of Copyright Offices, allowing registration of only highly original synthetic works, and help copyright owners signal the value of their works and facilitate fairer licensing deals. More generally, this approach offers valuable insights to policymakers grappling with adapting copyright law to the challenges posed by the era of GenAI.","Tue, 26 Mar 2024 13:32:32 UTC (10,819 KB)"
"18","Assessing the similarity of real matrices with arbitrary shape","Jasper Albers, Anno C. Kurth, Robin Gutzen, Aitor Morales-Gregorio, Michael Denker, Sonja Grün, Sacha J. van Albada, Markus Diesmann","Neurons and Cognition (q-bio.NC)","Assessing the similarity of matrices is valuable for analyzing the extent to which data sets exhibit common features in tasks such as data clustering, dimensionality reduction, pattern recognition, group comparison, and graph analysis. Methods proposed for comparing vectors, such as cosine similarity, can be readily generalized to matrices. However, this approach usually neglects the inherent two-dimensional structure of matrices. Here, we propose singular angle similarity (SAS), a measure for evaluating the structural similarity between two arbitrary, real matrices of the same shape based on singular value decomposition. After introducing the measure, we compare SAS with standard measures for matrix comparison and show that only SAS captures the two-dimensional structure of matrices. Further, we characterize the behavior of SAS in the presence of noise and as a function of matrix dimensionality. Finally, we apply SAS to two use cases: square non-symmetric matrices of probabilistic network connectivity, and non-square matrices representing neural brain activity. For synthetic data of network connectivity, SAS matches intuitive expectations and allows for a robust assessment of similarities and differences. For experimental data of brain activity, SAS captures differences in the structure of high-dimensional responses to different stimuli. We conclude that SAS is a suitable measure for quantifying the shared structure of matrices with arbitrary shape.","Tue, 26 Mar 2024 13:24:52 UTC (1,141 KB)"
"19","Analysis on reservoir activation with the nonlinearity harnessed from solution-processed MoS2 devices","Songwei Liu, Yang Liu, Yingyi Wen, Jingfang Pei, Pengyu Liu, Lekai Song, Xiaoyue Fan, Wenchen Yang, Danmei Pan, Teng Ma, Yue Lin, Gang Wang, Guohua Hu","Applied Physics (physics.app-ph)","Reservoir computing is a recurrent neural network that has been applied across various domains in machine learning. The implementation of reservoir computing, however, often demands heavy computations for activating the reservoir. Configuring physical reservoir networks and harnessing the nonlinearity from the underlying devices for activation is an emergent solution to address the computational challenge. Herein, we analyze the feasibility of employing the nonlinearity from solution-processed molybdenum disulfide (MoS2) devices for reservoir activation. The devices, fabricated using liquid-phase exfoliated MoS2, exhibit a high-order nonlinearity achieved by Stark modulation of the MoS2 material. We demonstrate that this nonlinearity can be fitted and employed as the activation function to facilitate reservoir computing implementation. Notably, owing to the high-order nonlinearity, the network exhibits long-term synchronization and robust generalization abilities for approximating complex dynamical systems. Given the remarkable reservoir activation capability, coupled with the scalability of the device fabrication, our findings open the possibility for the physical realization of lightweight, efficient reservoir computing for, for instance, signal classification, motion tracking, and pattern recognition of complex time series as well as secure cryptography. As an example, we show the network can be appointed to generate chaotic random numbers for secure data encryption.","Tue, 26 Mar 2024 13:04:00 UTC (2,609 KB)"
"20","How Private is DP-SGD?","Lynn Chua, Badih Ghazi, Pritish Kamath, Ravi Kumar, Pasin Manurangsi, Amer Sinha, Chiyuan Zhang","Machine Learning (cs.LG)","We demonstrate a substantial gap between the privacy guarantees of the Adaptive Batch Linear Queries (ABLQ) mechanism under different types of batch sampling: (i) Shuffling, and (ii) Poisson subsampling; the typical analysis of Differentially Private Stochastic Gradient Descent (DP-SGD) follows by interpreting it as a post-processing of ABLQ. While shuffling based DP-SGD is more commonly used in practical implementations, it is neither analytically nor numerically amenable to easy privacy analysis. On the other hand, Poisson subsampling based DP-SGD is challenging to scalably implement, but has a well-understood privacy analysis, with multiple open-source numerically tight privacy accountants available. This has led to a common practice of using shuffling based DP-SGD in practice, but using the privacy analysis for the corresponding Poisson subsampling version. Our result shows that there can be a substantial gap between the privacy analysis when using the two types of batch sampling, and thus advises caution in reporting privacy parameters for DP-SGD.","Tue, 26 Mar 2024 13:02:43 UTC (684 KB)"
"21","Constraining Primordial Non-Gaussianity from Large Scale Structure with the Wavelet Scattering Transform","Matteo Peron, Gabriel Jung, Michele Liguori, Massimo Pietroni","Cosmology and Nongalactic Astrophysics (astro-ph.CO)","We investigate the Wavelet Scattering Transform (WST) as a tool for the study of Primordial non-Gaussianity (PNG) in Large Scale Structure (LSS), and compare its performance with that achievable via a joint analysis with power spectrum and bispectrum (P+B). We consider the three main primordial bispectrum shapes - local, equilateral and orthogonal - and produce Fisher forecast for the corresponding fNL amplitude parameters, jointly with standard cosmological parameters. We analyze simulations from the publicly available ""Quijote"" and ""Quijote-png"" N-body suites, studying both the dark matter and halo fields. We find that the WST outperforms the power spectrum alone on all parameters, both on the fNL's and on cosmological ones. In particular, on fNL_loc for halos, the improvement is about 27%. When B is combined with P, halo constraints from WST are weaker for fNL_loc (at ~ 15% level), but stronger for fNL_eq (~ 25%) and fNL_ortho (~ 28%). Our results show that WST, both alone and in combination with P+B, can improve the extraction of information on PNG from LSS data over the one attainable by a standard P+B analysis. Moreover, we identify a class of WST in which the origin of the extra information on PNG can be cleanly isolated.","Tue, 26 Mar 2024 12:40:49 UTC (18,902 KB)"
"22","Microscale Morphology Driven Thermal Transport in Fiber Reinforced Polymer Composites","Sabarinathan P Subramaniyan, Jonathan D Boehnlein, Pavana Prabhakar","Applied Physics (physics.app-ph)","Fiber-reinforced polymer composite (FRPC) materials are used extensively in various industries, such as aerospace, automobiles, and electronics packaging, due to their remarkable specific strength and desirable properties, such as enhanced durability and corrosion resistance. The evolution of thermal properties in FRPCs is crucial for advancing thermal management systems, optimizing material performance, and enhancing energy efficiency across these diverse sectors. Despite significant research efforts to develop new materials with improved thermal properties and reduced thermal degradation, there is a lack of understanding of the thermal transport phenomena considering the influence of microscale reinforcement morphology in these composites. In the current study, we performed experimental investigations complemented by computations to determine the thermal transport properties and associated phenomena in epoxy and carbon fiber-reinforced epoxy composites. The experimental findings were utilized as input data for numerical analysis to examine the impact of fiber morphology and volume fraction in thermal transport phenomena. Our results revealed that composites incorporating non-circular fibers manifested higher thermal conductivity than traditional circular fibers in the transverse direction. This can be attributed to increased interconnected heat flow pathways facilitated by the increased surface area of non-circular fibers with the same cross-sectional areas, resulting in efficient heat transfer.","Tue, 26 Mar 2024 12:31:33 UTC (1,801 KB)"
"23","Data-driven Energy Consumption Modelling for Electric Micromobility using an Open Dataset","Yue Ding, Sen Yan, Maqsood Hussain Shah, Hongyuan Fang, Ji Li, Mingming Liu","Artificial Intelligence (cs.AI)","The escalating challenges of traffic congestion and environmental degradation underscore the critical importance of embracing E-Mobility solutions in urban spaces. In particular, micro E-Mobility tools such as E-scooters and E-bikes, play a pivotal role in this transition, offering sustainable alternatives for urban commuters. However, the energy consumption patterns for these tools are a critical aspect that impacts their effectiveness in real-world scenarios and is essential for trip planning and boosting user confidence in using these. To this effect, recent studies have utilised physical models customised for specific mobility tools and conditions, but these models struggle with generalization and effectiveness in real-world scenarios due to a notable absence of open datasets for thorough model evaluation and verification. To fill this gap, our work presents an open dataset, collected in Dublin, Ireland, specifically designed for energy modelling research related to E-Scooters and E-Bikes. Furthermore, we provide a comprehensive analysis of energy consumption modelling based on the dataset using a set of representative machine learning algorithms and compare their performance against the contemporary mathematical models as a baseline. Our results demonstrate a notable advantage for data-driven models in comparison to the corresponding mathematical models for estimating energy consumption. Specifically, data-driven models outperform physical models in accuracy by up to 83.83% for E-Bikes and 82.16% for E-Scooters based on an in-depth analysis of the dataset under certain assumptions.","Tue, 26 Mar 2024 12:08:05 UTC (23,196 KB)"
"24","Online Tree Reconstruction and Forest Inventory on a Mobile Robotic System","Leonard Freißmuth, Matias Mattamala, Nived Chebrolu, Simon Schaefer, Stefan Leutenegger, Maurice Fallon","Robotics (cs.RO)","Terrestrial laser scanning (TLS) is the standard technique used to create accurate point clouds for digital forest inventories. However, the measurement process is demanding, requiring up to two days per hectare for data collection, significant data storage, as well as resource-heavy post-processing of 3D data. In this work, we present a real-time mapping and analysis system that enables online generation of forest inventories using mobile laser scanners that can be mounted e.g. on mobile robots. Given incrementally created and locally accurate submaps-data payloads-our approach extracts tree candidates using a custom, Voronoi-inspired clustering algorithm. Tree candidates are reconstructed using an adapted Hough algorithm, which enables robust modeling of the tree stem. Further, we explicitly incorporate the incremental nature of the data collection by consistently updating the database using a pose graph LiDAR SLAM system. This enables us to refine our estimates of the tree traits if an area is revisited later during a mission. We demonstrate competitive accuracy to TLS or manual measurements using laser scanners that we mounted on backpacks or mobile robots operating in conifer, broad-leaf and mixed forests. Our results achieve RMSE of 1.93 cm, a bias of 0.65 cm and a standard deviation of 1.81 cm (averaged across these sequences)-with no post-processing required after the mission is complete.","Tue, 26 Mar 2024 11:51:58 UTC (5,709 KB)"
"25","Lattice dynamics of LiNb$_{\text{1-x}}$Ta$_{\text{x}}$O$_{\text{3}}$ solid solutions: Theory and experiment","Felix Bernhardt, Soham Gharat, Alexander Kapp, Florian Pfeiffer, Robin Buschbeck, Franz Hempel, Oleksiy Pashkin, Susanne C. Kehr, Michael Rüsing, Simone Sanna, Lukas M. Eng","Materials Science (cond-mat.mtrl-sci)","Lithium niobate (LNO) and lithium tantalate (LTO) see widespread use in fundamental research and commercial technologies reaching from electronics over classical optics to integrated quantum communication. In recent years, the mixed crystal system lithium niobate tantalate (LNT) allows for the dedicate engineering of material properties by combining the advantages of the two parental materials LNO and LTO. Vibrational spectroscopies such as Raman spectroscopy or (Fourier transform) infrared spectroscopy are vital techniques to provide detailed insight into the material properties, which is central to the analysis and optimization of devices. In this work, we present a joint experimental-theoretical approach allowing to unambiguously assign the spectral features in the LNT material family through both Raman and IR spectroscopy, as well as to provide an in-depth explanation for the observed scattering efficiencies based on first-principles calculations. The phononic contribution to the static dielectric tensor is calculated from the experimental and theoretical data using the generalized Lyddane-Sachs-Teller relation and compared with the results of the first-principles calculations. The joint methodology can be readily expanded to other materials and serves, e.g., as the basis for studying the role of point defects or doping.","Tue, 26 Mar 2024 11:05:32 UTC (2,245 KB)"
"26","Parameterized Analysis of Bribery in Challenge the Champ Tournaments","Juhi Chaudhary, Hendrik Molter, Meirav Zehavi","Data Structures and Algorithms (cs.DS)","Challenge the champ tournaments are one of the simplest forms of competition, where a (initially selected) champ is repeatedly challenged by other players. If a player beats the champ, then that player is considered the new (current) champ. Each player in the competition challenges the current champ once in a fixed order. The champ of the last round is considered the winner of the tournament. We investigate a setting where players can be bribed to lower their winning probability against the initial champ. The goal is to maximize the probability of the initial champ winning the tournament by bribing the other players, while not exceeding a given budget for the bribes. Mattei et al. [Journal of Applied Logic, 2015] showed that the problem can be solved in pseudo-polynomial time, and that it is in XP when parameterized by the number of players.
We show that the problem is weakly NP-hard and W[1]-hard when parameterized by the number of players. On the algorithmic side, we show that the problem is fixed-parameter tractable when parameterized either by the number of different bribe values or the number of different probability values. To this end, we establish several results that are of independent interest. In particular, we show that the product knapsack problem is W[1]-hard when parameterized by the number of items in the knapsack, and that constructive bribery for cup tournaments is W[1]-hard when parameterized by the number of players. Furthermore, we present a novel way of designing mixed integer linear programs, ensuring optimal solutions where all variables are integers.","Tue, 26 Mar 2024 10:53:25 UTC (60 KB)"
"27","A Survey on Deep Learning and State-of-the-arts Applications","Mohd Halim Mohd Noor, Ayokunle Olalekan Ige","Machine Learning (cs.LG)","Deep learning, a branch of artificial intelligence, is a computational model that uses multiple layers of interconnected units (neurons) to learn intricate patterns and representations directly from raw input data. Empowered by this learning capability, it has become a powerful tool for solving complex problems and is the core driver of many groundbreaking technologies and innovations. Building a deep learning model is a challenging task due to the algorithm`s complexity and the dynamic nature of real-world problems. Several studies have reviewed deep learning concepts and applications. However, the studies mostly focused on the types of deep learning models and convolutional neural network architectures, offering limited coverage of the state-of-the-art of deep learning models and their applications in solving complex problems across different domains. Therefore, motivated by the limitations, this study aims to comprehensively review the state-of-the-art deep learning models in computer vision, natural language processing, time series analysis and pervasive computing. We highlight the key features of the models and their effectiveness in solving the problems within each domain. Furthermore, this study presents the fundamentals of deep learning, various deep learning model types and prominent convolutional neural network architectures. Finally, challenges and future directions in deep learning research are discussed to offer a broader perspective for future researchers.","Tue, 26 Mar 2024 10:10:53 UTC (1,990 KB)"
"28","Practical Applications of Advanced Cloud Services and Generative AI Systems in Medical Image Analysis","Jingyu Xu, Binbin Wu, Jiaxin Huang, Yulu Gong, Yifan Zhang, Bo Liu","Artificial Intelligence (cs.AI)","The medical field is one of the important fields in the application of artificial intelligence technology. With the explosive growth and diversification of medical data, as well as the continuous improvement of medical needs and challenges, artificial intelligence technology is playing an increasingly important role in the medical field. Artificial intelligence technologies represented by computer vision, natural language processing, and machine learning have been widely penetrated into diverse scenarios such as medical imaging, health management, medical information, and drug research and development, and have become an important driving force for improving the level and quality of medical services.The article explores the transformative potential of generative AI in medical imaging, emphasizing its ability to generate syntheticACM-2 data, enhance images, aid in anomaly detection, and facilitate image-to-image translation. Despite challenges like model complexity, the applications of generative models in healthcare, including Med-PaLM 2 technology, show promising results. By addressing limitations in dataset size and diversity, these models contribute to more accurate diagnoses and improved patient outcomes. However, ethical considerations and collaboration among stakeholders are essential for responsible implementation. Through experiments leveraging GANs to augment brain tumor MRI datasets, the study demonstrates how generative AI can enhance image quality and diversity, ultimately advancing medical diagnostics and patient care.","Tue, 26 Mar 2024 09:55:49 UTC (503 KB)"
"29","Determination of nuclear matter radii by means of microscopic optical potentials: the case of $^{78}$Kr","Matteo Vorabbi, Paolo Finelli, Carlotta Giusti","Nuclear Theory (nucl-th)","In this work we use microscopic Nucleon-Nucleus Optical Potentials (OP) to analyze elastic scattering data for the differential cross section of the $^{78}$Kr (p,p) $^{78}$Kr reaction, with the goal of extracting the matter radius and estimating the neutron skin, quantities that are both needed to determine the slope parameter $L$ of the nuclear symmetry energy. Our analysis is performed with the factorized version of the microscopic OP obtained in a previous series of papers within the Watson multiple scattering theory at the first order of the spectator expansion, which is based on the underlying nucleon-nucleon dynamics and is free from phenomenological inputs. Differently from our previous applications, the proton and neutron densities are described with a two-parameter Fermi (2pF) distribution, which makes the extraction of the matter radius easier and allows us to make a meaningful comparison with the original analysis, that was performed with the Glauber model. With standard minimization techniques we performed data analysis and extracted the matter radius and the neutron skin. Our analysis produces a matter radius of $R_m^{\rm (rms)} = 4.12$ fm, in good agreement with previous matter radii extracted from $^{76}$Kr and $^{80}$Kr, and a neutron skin of $\Delta R_{np} \simeq - 0.1$ fm, compatible with a previous analysis. Our factorized microscopic OP, supplied with 2pF densities, is a valuable tool to perform the analysis of the experimental differential cross section and extract information such as matter radius and neutron skin. Without any free parameters it provides a reasonably good description of the experimental differential cross section for scattering angles up to $\approx$ 40 degrees. Compared to the Glauber model our OP can be applied to a wider range of scattering angles and allows one to probe the nuclear systems in a more internal region.","Tue, 26 Mar 2024 09:46:56 UTC (105 KB)"
"30","Prediction-sharing During Training and Inference","Yotam Gafni, Ronen Gradwohl, Moshe Tennenholtz","Theoretical Economics (econ.TH)","Two firms are engaged in a competitive prediction task. Each firm has two sources of data -- labeled historical data and unlabeled inference-time data -- and uses the former to derive a prediction model, and the latter to make predictions on new instances. We study data-sharing contracts between the firms. The novelty of our study is to introduce and highlight the differences between contracts that share prediction models only, contracts to share inference-time predictions only, and contracts to share both. Our analysis proceeds on three levels. First, we develop a general Bayesian framework that facilitates our study. Second, we narrow our focus to two natural settings within this framework: (i) a setting in which the accuracy of each firm's prediction model is common knowledge, but the correlation between the respective models is unknown; and (ii) a setting in which two hypotheses exist regarding the optimal predictor, and one of the firms has a structural advantage in deducing it. Within these two settings we study optimal contract choice. More specifically, we find the individually rational and Pareto-optimal contracts for some notable cases, and describe specific settings where each of the different sharing contracts emerge as optimal. Finally, in the third level of our analysis we demonstrate the applicability of our concepts in a synthetic simulation using real loan data.","Tue, 26 Mar 2024 09:18:50 UTC (1,011 KB)"
"31","baskexact: An R package for analytical calculation of basket trial operating characteristics","Lukas Baumann","Computation (stat.CO)","Basket trials are a new type of clinical trial in which a treatment is investigated in several subgroups. For the analysis of these trials, information is shared between the subgroups based on the observed data to increase the power. Many approaches for the analysis of basket trials have been suggested, but only a few have been implemented in open source software packages. The R package baskexact facilitates the evaluation of two basket trial designs which use empirical Bayes techniques for sharing information. With baskexact, operating characteristics for single-stage and two-stage designs can be calculated analytically and optimal tuning parameters can be selected.","Tue, 26 Mar 2024 09:11:58 UTC (240 KB)"
"32","Algorithmic unfolding for image reconstruction and localization problems in fluorescence microscopy","Silvia Bonettini, Luca Calatroni, Danilo Pezzi, Marco Prato","Numerical Analysis (math.NA)","We propose an unfolded accelerated projected-gradient descent procedure to estimate model and algorithmic parameters for image super-resolution and molecule localization problems in image microscopy. The variational lower-level constraint enforces sparsity of the solution and encodes different noise statistics (Gaussian, Poisson), while the upper-level cost assesses optimality w.r.t.~the task considered. In more detail, a standard $\ell_2$ cost is considered for image reconstruction (e.g., deconvolution/super-resolution, semi-blind deconvolution) problems, while a smoothed $\ell_1$ is employed to assess localization precision in some exemplary fluorescence microscopy problems exploiting single-molecule activation. Several numerical experiments are reported to validate the proposed approach on synthetic and realistic ISBI data.","Tue, 26 Mar 2024 09:08:25 UTC (3,044 KB)"
"33","A Type of Nonlinear Fréchet Regressions","Lu Lin, Ze Chen","Methodology (stat.ME)","The existing Fréchet regression is actually defined within a linear framework, since the weight function in the Fréchet objective function is linearly defined, and the resulting Fréchet regression function is identified to be a linear model when the random object belongs to a Hilbert space. Even for nonparametric and semiparametric Fréchet regressions, which are usually nonlinear, the existing methods handle them by local linear (or local polynomial) technique, and the resulting Fréchet regressions are (locally) linear as well. We in this paper introduce a type of nonlinear Fréchet regressions. Such a framework can be utilized to fit the essentially nonlinear models in a general metric space and uniquely identify the nonlinear structure in a Hilbert space. Particularly, its generalized linear form can return to the standard linear Fréchet regression through a special choice of the weight function. Moreover, the generalized linear form possesses methodological and computational simplicity because the Euclidean variable and the metric space element are completely separable. The favorable theoretical properties (e.g. the estimation consistency and presentation theorem) of the nonlinear Fréchet regressions are established systemically. The comprehensive simulation studies and a human mortality data analysis demonstrate that the new strategy is significantly better than the competitors.","Tue, 26 Mar 2024 08:23:37 UTC (66 KB)"
"34","Investigations on Physics-Informed Neural Networks for Aerodynamics","Guillaume Coulaud (ACUMES), Maxime Le (ACUMES), Régis Duvigneau (ACUMES)","Analysis of PDEs (math.AP)","Physics-Informed Neural Networks (PINNs) have recently emerged as a novel approach to simulate complex physical systems on the basis of both data observations and physical models. In this work, we investigate the use of PINNs for various applications in aerodynamics and we explain how to leverage their specific formulation to perform some tasks effectively. In particular, we demonstrate the ability of PINNs to construct parametric surrogate models, to achieve multiphysic couplings and to infer turbulence characteristics via data assimilation. The robustness and accuracy of the PINNs approach are analysed, then current issues and challenges are discussed.","Tue, 26 Mar 2024 07:57:56 UTC (1,234 KB)"
"35","Green HPC: An analysis of the domain based on Top500","Abdessalam Benhari (LIG, DATAMOVE ), Denis Trystram (UGA, DATAMOVE ), Fanny Dufossé (DATAMOVE), Yves Denneulin, Frédéric Desprez","Computers and Society (cs.CY)","The demand in computing power has never stopped growing over the years. Today, the performance of the most powerful systems exceeds the exascale and the number of petascale systems continues to grow. Unfortunately, this growth also goes hand in hand with ever-increasing energy costs, which in turn means a significant carbon footprint. In view of the environmental crisis, this paper intents to look at the often hidden issue of energy consumption of HPC systems. As it is not easy to access the data of the constructors, we then consider the Top500 as the tip of the iceberg to identify the trends of the whole domain.The objective of this work is to analyze Top500 and Green500 data from several perspectives in order to identify the dynamic of the domain regarding its environmental impact. The contributions are to take stock of the empirical laws governing the evolution of HPC computing systems both from the performance and energy perspectives, to analyze the most relevant data for developing the performance and energy efficiency of large-scale computing systems, to put these analyses into perspective with effects and impacts (lifespan of the HPC systems) and finally to derive a predictive model for the weight of HPC sector within the horizon 2030.","Tue, 26 Mar 2024 07:55:40 UTC (1,105 KB)"
"36","Localized Inverse Design in Conservation Laws and Hamilton-Jacobi Equations","Rinaldo M. Colombo, Vincent Perrollaz (IDP)","Analysis of PDEs (math.AP)","Consider the inverse design problem for a scalar conservation law, i.e., the problem of finding initial data evolving into a given profile at a given time. The solution we present below takes into account localizations both in the final interval where the profile is assigned and in the initial interval where the datum is sought, as well as additional a priori constraints on the datum's range provided by the model. These results are motivated and can be applied to data assimilation procedures in traffic modeling and accidents localization.","Tue, 26 Mar 2024 07:53:28 UTC (21 KB)"
"37","Rapid neutron star equation of state inference with Normalising Flows","Jordan McGinn, Arunava Mukherjee, Jessica Irwin, Christopher Messenger, Michael J. Williams, Ik Siong Heng","General Relativity and Quantum Cosmology (gr-qc)","The first direct detection of gravitational waves from binary neutron stars on the 17th of August, 2017, (GW170817) heralded the arrival of a new messenger for probing neutron star astrophysics and provided the first constraints on neutron star equation of state from gravitational wave observations. Significant computational effort was expended to obtain these first results and therefore, as observations of binary neutron star coalescence become more routine in the coming observing runs, there is a need to improve the analysis speed and flexibility. Here, we present a rapid approach for inferring the neutron star equation of state based on Normalising Flows. As a demonstration, using the same input data, our approach, ASTREOS, produces results consistent with those presented by the LIGO-Virgo collaboration but requires < 1 sec to generate neutron star equation of state confidence intervals. Furthermore, ASTREOS allows for non-parametric equation of state inference. This rapid analysis will not only facilitate neutron star equation of state studies but can potentially enhance future alerts for electromagnetic follow-up observations of binary neutron star mergers.","Tue, 26 Mar 2024 07:53:21 UTC (2,333 KB)"
"38","Study on high-frequency quasi-periodic oscillations in rotating black bounce spacetime","Jianbo Lu, Shining Yang, Mou Xu, Liu Yang","General Relativity and Quantum Cosmology (gr-qc)","We investigate dynamical effects of particles moving around rotating SV regular BH and traversable wormholes, and the deviation from Kerr BH. We found that the rotating SV traversable wormhole spacetime has a closer innermost stable circular orbit to the central object than Kerr and rotating regular BHs. Furthermore, the paper provides the general form of the epicycle frequency in this spacetime, presenting the radial profile of the angular frequency of particles undergoing oscillatory motion. Unlike the commonly used $\chi$-square analysis method, with considering a generalized data analysis approach in this paper we hypothesize that the observational data of three microguasars can be explained through an axisymmetric SV spacetime, fitting the data to find the range of spin values for three types of SV objects, and analyzing the possible mechanisms of HFQPOs (high-frequency quasi-periodic oscillations) generation for different types of axisymmetric SV objects. We found that for a Kerr BH with $l^*=0$, the observational data of three microquasars can be explained by the HFQPOs model used in this paper (excluding ER4 model); for a regular BH with $l^*=1$, the HFQPO phenomenon observed through the three microquasars can be explained by the ER0, ER1, ER2, ER3, RP0, RP1, RP2, TD and WD models. For a traversable wormhole with $l^*=3$, the mechanism of HFQPO can be interpreted by the ER1, ER3, RP0, RP1, RP2, TD and WD models. Moreover, we observed that for the same HFQPOs model, the limiting range of the spin parameter $\alpha^*$ increases with the increase of the parameter $l^*$.","Tue, 26 Mar 2024 07:40:47 UTC (837 KB)"
"39","Cost-benefit analysis of ecosystem modelling to support fisheries management","Matthew H. Holden, Eva E. Plagányi, Elizabeth A. Fulton, Alexander B. Campbell, Rachel Janes, Robyn A. Lovett, Montana Wickens, Matthew P. Adams, Larissa Lubiana Botelho, Catherine M. Dichmont, Philip Erm, Kate J Helmstedt, Ryan F. Heneghan, Manuela Mendiolar, Anthony J. Richardson, Jacob G. D. Rogers, Kate Saunders, Liam Timms","Populations and Evolution (q-bio.PE)","Mathematical and statistical models underlie many of the world's most important fisheries management decisions. Since the 19th century, difficulty calibrating and fitting such models has been used to justify the selection of simple, stationary, single-species models to aid tactical fisheries management decisions. Whereas these justifications are reasonable, it is imperative that we quantify the value of different levels of model complexity for supporting fisheries management, especially given a changing climate, where old methodologies may no longer perform as well as in the past. Here we argue that cost-benefit analysis is an ideal lens to assess the value of model complexity in fisheries management. While some studies have reported the benefits of model complexity in fisheries, modeling costs are rarely considered. In the absence of cost data in the literature, we report, as a starting point, relative costs of single-species stock assessment and marine ecosystem models from two Australian organizations. We found that costs varied by two orders of magnitude, and that ecosystem model costs increased with model complexity. Using these costs, we walk through a hypothetical example of cost-benefit analysis. The demonstration is intended to catalyze the reporting of modeling costs and benefits.","Tue, 26 Mar 2024 07:24:28 UTC (474 KB)"
"40","Particle identification with machine learning from incomplete data in the ALICE experiment","Maja Karwowska, Łukasz Graczykowski, Kamil Deja, Miłosz Kasak, Małgorzata Janik (for the ALICE collaboration)","High Energy Physics - Experiment (hep-ex)","The ALICE experiment at the LHC measures properties of the strongly interacting matter formed in ultrarelativistic heavy-ion collisions. Such studies require accurate particle identification (PID). ALICE provides PID information via several detectors for particles with momentum from about 100 MeV/c up to 20 GeV/c. Traditionally, particles are selected with rectangular cuts. Acmuch better performance can be achieved with machine learning (ML) methods. Our solution uses multiple neural networks (NN) serving as binary classifiers. Moreover, we extended our particle classifier with Feature Set Embedding and attention in order to train on data with incomplete samples. We also present the integration of the ML project with the ALICE analysis software, and we discuss domain adaptation, the ML technique needed to transfer the knowledge between simulated and real experimental data.","Tue, 26 Mar 2024 07:05:06 UTC (1,608 KB)"
"41","Coupling-Constant Averaged Exchange-Correlation Hole for He, Li, Be, N, Ne Atoms from CCSD","Lin Hou, Tom J. P. Irons, Yanyong Wang, James W. Furness, Andrew M. Wibowo-Teale, Jianwei Sun","Chemical Physics (physics.chem-ph)","Accurate approximation of the exchange-correlation (XC) energy in density functional theory (DFT) calculations is essential for reliably modelling electronic systems. Many such approximations are developed from models of the XC hole; accurate reference XC holes for real electronic systems are crucial for evaluating the accuracy of these models however the availability of reliable reference data is limited to a few systems. In this study, we employ the Lieb optimization with a coupled cluster singles and doubles (CCSD) reference to construct accurate coupling-constant averaged XC holes, resolved into individual exchange and correlation components, for five spherically symmetric atoms: He, Li, Be, N, and Ne. Alongside providing a new set of reference data for the construction and evaluation of model XC holes, we compare our data against the exchange and correlation hole models of the established LDA and PBE density functional approximations. Our analysis confirms the established rationalization for the limitations of LDA and the improvement observed with PBE in terms of the hole depth and its long-range decay, demonstrated in real-space for the series of spherically-symmetric atoms.","Tue, 26 Mar 2024 06:42:09 UTC (535 KB)"
"42","Neural Clustering based Visual Representation Learning","Guikun Chen, Xia Li, Yi Yang, Wenguan Wang","Computer Vision and Pattern Recognition (cs.CV)","We investigate a fundamental aspect of machine vision: the measurement of features, by revisiting clustering, one of the most classic approaches in machine learning and data analysis. Existing visual feature extractors, including ConvNets, ViTs, and MLPs, represent an image as rectangular regions. Though prevalent, such a grid-style paradigm is built upon engineering practice and lacks explicit modeling of data distribution. In this work, we propose feature extraction with clustering (FEC), a conceptually elegant yet surprisingly ad-hoc interpretable neural clustering framework, which views feature extraction as a process of selecting representatives from data and thus automatically captures the underlying data distribution. Given an image, FEC alternates between grouping pixels into individual clusters to abstract representatives and updating the deep features of pixels with current representatives. Such an iterative working mechanism is implemented in the form of several neural layers and the final representatives can be used for downstream tasks. The cluster assignments across layers, which can be viewed and inspected by humans, make the forward process of FEC fully transparent and empower it with promising ad-hoc interpretability. Extensive experiments on various visual recognition models and tasks verify the effectiveness, generality, and interpretability of FEC. We expect this work will provoke a rethink of the current de facto grid-style paradigm.","Tue, 26 Mar 2024 06:04:50 UTC (1,157 KB)"
"43","Generalization Error Analysis for Sparse Mixture-of-Experts: A Preliminary Study","Jinze Zhao, Peihao Wang, Zhangyang Wang","Machine Learning (cs.LG)","Mixture-of-Experts (MoE) represents an ensemble methodology that amalgamates predictions from several specialized sub-models (referred to as experts). This fusion is accomplished through a router mechanism, dynamically assigning weights to each expert's contribution based on the input data. Conventional MoE mechanisms select all available experts, incurring substantial computational costs. In contrast, Sparse Mixture-of-Experts (Sparse MoE) selectively engages only a limited number, or even just one expert, significantly reducing computation overhead while empirically preserving, and sometimes even enhancing, performance. Despite its wide-ranging applications and these advantageous characteristics, MoE's theoretical underpinnings have remained elusive. In this paper, we embark on an exploration of Sparse MoE's generalization error concerning various critical factors. Specifically, we investigate the impact of the number of data samples, the total number of experts, the sparsity in expert selection, the complexity of the routing mechanism, and the complexity of individual experts. Our analysis sheds light on \textit{how \textbf{sparsity} contributes to the MoE's generalization}, offering insights from the perspective of classical learning theory.","Tue, 26 Mar 2024 05:48:02 UTC (85 KB)"
"44","Handling multivariable missing data in causal mediation analysis","S. Ghazaleh Dashti, Katherine J. Lee, Julie A. Simpson, John B. Carlin, Margarita Moreno-Betancur","Applications (stat.AP)","Mediation analysis is commonly used in epidemiological research, but guidance is lacking on how multivariable missing data should be dealt with in these analyses. Multiple imputation (MI) is a widely used approach, but questions remain regarding impact of missingness mechanism, how to ensure imputation model compatibility and approaches to variance estimation. To address these gaps, we conducted a simulation study based on the Victorian Adolescent Health Cohort Study. We considered six missingness mechanisms, involving varying assumptions regarding the influence of outcome and/or mediator on missingness in key variables. We compared the performance of complete-case analysis, seven MI approaches, differing in how the imputation model was tailored, and a ""substantive model compatible"" MI approach. We evaluated both the MI-Boot (MI, then bootstrap) and Boot-MI (bootstrap, then MI) approaches to variance estimation. Results showed that when the mediator and/or outcome influenced their own missingness, there was large bias in effect estimates, while for other mechanisms appropriate MI approaches yielded approximately unbiased estimates. Beyond incorporating all analysis variables in the imputation model, how MI was tailored for compatibility with mediation analysis did not greatly impact point estimation bias. BootMI returned variance estimates with smaller bias than MIBoot, especially in the presence of incompatibility.","Tue, 26 Mar 2024 05:25:29 UTC (2,062 KB)"
"45","Explainable Graph Neural Networks for Observation Impact Analysis in Atmospheric State Estimation","Hyeon-Ju Jeon, Jeon-Ho Kang, In-Hyuk Kwon, O-Joun Lee","Artificial Intelligence (cs.AI)","This paper investigates the impact of observations on atmospheric state estimation in weather forecasting systems using graph neural networks (GNNs) and explainability methods. We integrate observation and Numerical Weather Prediction (NWP) points into a meteorological graph, extracting $k$-hop subgraphs centered on NWP points. Self-supervised GNNs are employed to estimate the atmospheric state by aggregating data within these $k$-hop radii. The study applies gradient-based explainability methods to quantify the significance of different observations in the estimation process. Evaluated with data from 11 satellite and land-based observations, the results highlight the effectiveness of visualizing the importance of observation types, enhancing the understanding and optimization of observational data in weather forecasting.","Tue, 26 Mar 2024 05:10:47 UTC (1,554 KB)"
"46","Exploring and Applying Audio-Based Sentiment Analysis in Music","Etash Jhanji","Sound (cs.SD)","Sentiment analysis is a continuously explored area of text processing that deals with the computational analysis of opinions, sentiments, and subjectivity of text. However, this idea is not limited to text and speech, in fact, it could be applied to other modalities. In reality, humans do not express themselves in text as deeply as they do in music. The ability of a computational model to interpret musical emotions is largely unexplored and could have implications and uses in therapy and musical queuing. In this paper, two individual tasks are addressed. This study seeks to (1) predict the emotion of a musical clip over time and (2) determine the next emotion value after the music in a time series to ensure seamless transitions. Utilizing data from the Emotions in Music Database, which contains clips of songs selected from the Free Music Archive annotated with levels of valence and arousal as reported on Russel's circumplex model of affect by multiple volunteers, models are trained for both tasks. Overall, the performance of these models reflected that they were able to perform the tasks they were designed for effectively and accurately.","Thu, 22 Feb 2024 22:34:06 UTC (4,723 KB)"
"47","On the Heating of the Slow Solar-Wind by Imbalanced Alfvén-Wave Turbulence from 0.06 au to 1 au: Parker Solar Probe and Solar Orbiter observations","Sofiane Bourouaine, Jean C. Perez, Benjamin D. G. Chandran, Vamsee K. Jagarlamudi, Nour E. Raouafi, Jasper S. Halekas","Space Physics (physics.space-ph)","In this work we analyze plasma and magnetic field data provided by the Parker Solar Probe (\emph{PSP}) and Solar Orbiter (\emph{SO}) missions to investigate the radial evolution of the heating of Alfvénic slow wind (ASW) by imbalanced Alfvén-Wave (AW) turbulent fluctuations from 0.06 au to 1 au. in our analysis we focus on slow solar-wind intervals with highly imbalanced and incompressible turbulence (i.e., magnetic compressibility $C_B=\delta B/B\leq 0.25$, plasma compressibility $C_n=\delta n/n\leq 0.25$ and normalized cross-helicity $\sigma_c\geq 0.65$). First, we estimate the AW turbulent dissipation rate from the wave energy equation and find that the radial profile trend is similar to the proton heating rate. Second, we find that the scaling of the empirical AW turbulent dissipation rate $Q_W$ obtained from the wave energy equation matches the scaling from the phenomenological AW turbulent dissipation rate $Q_{\rm CH09}$ (with $Q_{\rm CH09}\simeq 1.55 Q_W$) derived by~\cite{chandran09} based on the model of reflection-driven turbulence. Our results suggest that, as in the fast solar wind, AW turbulence plays a major role in the ion heating that occurs in incompressible slow-wind streams.","Tue, 26 Mar 2024 03:31:24 UTC (543 KB)"
"48","Measurement Uncertainty Impact on Koopman Operator Estimation of Power System Dynamics","P. Algikar, P. Sharma, M. Netto, L. Mili","Applications (stat.AP)","Sensor measurements are mission-critical for monitoring and controlling power systems because they provide real-time insight into the grid operating condition; however, confidence in these insights depends greatly on the quality of the sensor data. Uncertainty in sensor measurements is an intrinsic aspect of the measurement process. In this paper, we develop an analytical method to quantify the impact of measurement uncertainties in numerical methods that employ the Koopman operator to identify nonlinear dynamics based on recorded data. In particular, we quantify the confidence interval of each element in the push-forward matrix from which a subset of the Koopman operator's discrete spectrum is estimated. We provide a detailed numerical analysis of the developed method applied to numerical simulations and field data collected from experiments conducted in a megawatt-scale facility at the National Renewable Energy Laboratory.","Tue, 26 Mar 2024 02:53:26 UTC (4,822 KB)"
"49","Labeling subtypes in a Parkinson's Cohort using Multifeatures in MRI - Integrating Grey and White Matter Information","Tanmayee Samantaray, Jitender Saini, Pramod Kumar Pal, Bithiah Grace Jaganathan, Vijaya V Saradhi, Gupta CN","Image and Video Processing (eess.IV)","Thresholding of networks has long posed a challenge in brain connectivity analysis. Weighted networks are typically binarized using threshold measures to facilitate network analysis. Previous studies on MRI-based brain networks have predominantly utilized density or sparsity-based thresholding techniques, optimized within specific ranges derived from network metrics such as path length, clustering coefficient, and small-world index. Thus, determination of a single threshold value for facilitating comparative analysis of networks remains elusive. To address this, our study introduces Mutual K-Nearest Neighbor (MKNN)-based thresholding for brain network analysis. Here, nearest neighbor selection is based on the highest correlation between features of brain regions. Construction of brain networks was accomplished by computing Pearson correlations between grey matter volume and white matter volume for each pair of brain regions. Structural MRI data from 180 Parkinsons patients and 70 controls from the NIMHANS, India were analyzed. Subtypes within Parkinsons disease were identified based on grey and white matter volume atrophy using source-based morphometric decomposition. The loading coefficients were correlated with clinical features to discern clinical relationship with the deciphered subtypes. Our data-mining approach revealed: Subtype A (N = 51, intermediate type), Subtype B (N = 57, mild-severe type with mild motor symptoms), and Subtype AB (N = 36, most-severe type with predominance in motor impairment). Subtype-specific weighted matrices were binarized using MKNN-based thresholding for brain network analysis. Permutation tests on network metrics of resulting bipartite graphs demonstrated significant group differences in betweenness centrality and participation coefficient. The identified hubs were specific to each subtype, with some hubs conserved across different subtypes.","Tue, 26 Mar 2024 02:32:52 UTC (2,741 KB)"
"50","FedMIL: Federated-Multiple Instance Learning for Video Analysis with Optimized DPP Scheduling","Ashish Bastola, Hao Wang, Xiwen Chen, Abolfazl Razi","Distributed, Parallel, and Cluster Computing (cs.DC)","Many AI platforms, including traffic monitoring systems, use Federated Learning (FL) for decentralized sensor data processing for learning-based applications while preserving privacy and ensuring secured information transfer. On the other hand, applying supervised learning to large data samples, like high-resolution images requires intensive human labor to label different parts of a data sample. Multiple Instance Learning (MIL) alleviates this challenge by operating over labels assigned to the 'bag' of instances. In this paper, we introduce Federated Multiple-Instance Learning (FedMIL). This framework applies federated learning to boost the training performance in video-based MIL tasks such as vehicle accident detection using distributed CCTV networks. However, data sources in decentralized settings are not typically Independently and Identically Distributed (IID), making client selection imperative to collectively represent the entire dataset with minimal clients. To address this challenge, we propose DPPQ, a framework based on the Determinantal Point Process (DPP) with a quality-based kernel to select clients with the most diverse datasets that achieve better performance compared to both random selection and current DPP-based client selection methods even with less data utilization in the majority of non-IID cases. This offers a significant advantage for deployment on edge devices with limited computational resources, providing a reliable solution for training AI models in massive smart sensor networks.","Tue, 26 Mar 2024 02:30:50 UTC (4,187 KB)"
"51","Testing the $\mathbfΛ$CDM Cosmological Model with Forthcoming Measurements of the Cosmic Microwave Background with SPT-3G","K. Prabhu, S. Raghunathan, M. Millea, G. Lynch, P. A. R. Ade, E. Anderes, A. J. Anderson, B. Ansarinejad, M. Archipley, L. Balkenhol, K. Benabed, A. N. Bender, B. A. Benson, F. Bianchini, L. E. Bleem, F. R. Bouchet, L. Bryant, E. Camphuis, J. E. Carlstrom, T. W. Cecil, C. L. Chang, P. Chaubal, P. M. Chichura, T.-L. Chou, A. Coerver, T. M. Crawford, A. Cukierman, C. Daley, T. de Haan, K. R. Dibert, M. A. Dobbs, A. Doussot, D. Dutcher, W. Everett, C. Feng, K. R. Ferguson, K. Fichman, A. Foster, S. Galli, A. E. Gambrel, R. W. Gardner, F. Ge, N. Goeckner-Wald, R. Gualtieri, F. Guidi, S. Guns, N. W. Halverson, E. Hivon, G. P. Holder, W. L. Holzapfel, J. C. Hood, A. Hryciuk, N. Huang, F. Kéruzoré, L. Knox, M. Korman, K. Kornoelje, C.-L. Kuo, A. T. Lee, K. Levy, A. E. Lowitz, C. Lu, A. Maniyar, F. Menanteau, J. Montgomery, Y. Nakato, T. Natoli, G. I. Noble, V. Novosad, Y. Omori, S. Padin, Z. Pan, P. Paschos, K. A. Phadke, W. Quan, M. Rahimi, A. Rahlin, C. L. Reichardt, M. Rouble, J. E. Ruhl, E. Schiappucci, G. Smecher, J. A. Sobrin, A. A. Stark, J. Stephen, A. Suzuki, C. Tandoi, K. L. Thompson, B. Thorne, C. Trendafilova, C. Tucker, C. Umilta, A. Vitrier, J. D. Vieira, Y. Wan, G. Wang, N. Whitehorn, W. L. K. Wu, V. Yefremenko, M. R. Young, J. A. Zebrowski","Cosmology and Nongalactic Astrophysics (astro-ph.CO)","We forecast constraints on cosmological parameters enabled by three surveys conducted with SPT-3G, the third-generation camera on the South Pole Telescope. The surveys cover separate regions of 1500, 2650, and 6000 ${\rm deg}^{2}$ to different depths, in total observing 25% of the sky. These regions will be measured to white noise levels of roughly 2.5, 9, and 12 $\mu{\rm K-arcmin}$, respectively, in CMB temperature units at 150 GHz by the end of 2024. The survey also includes measurements at 95 and 220 GHz, which have noise levels a factor of ~1.2 and 3.5 times higher than 150 GHz, respectively, with each band having a polarization noise level ~$\sqrt{\text{2}}$ times higher than the temperature noise. We use a novel approach to obtain the covariance matrices for jointly and optimally estimated gravitational lensing potential bandpowers and unlensed CMB temperature and polarization bandpowers. We demonstrate the ability to test the $\Lambda{\rm CDM}$ model via the consistency of cosmological parameters constrained independently from SPT-3G and Planck data, and consider the improvement in constraints on $\Lambda{\rm CDM}$ extension parameters from a joint analysis of SPT-3G and Planck data. The $\Lambda{\rm CDM}$ cosmological parameters are typically constrained with uncertainties up to ~2 times smaller with SPT-3G data, compared to Planck, with the two data sets measuring significantly different angular scales and polarization levels, providing additional tests of the standard cosmological model.","Tue, 26 Mar 2024 17:57:20 UTC (4,338 KB)"
"52","Empowering Data Mesh with Federated Learning","Haoyuan Li, Salman Toor","Machine Learning (cs.LG)","The evolution of data architecture has seen the rise of data lakes, aiming to solve the bottlenecks of data management and promote intelligent decision-making. However, this centralized architecture is limited by the proliferation of data sources and the growing demand for timely analysis and processing. A new data paradigm, Data Mesh, is proposed to overcome these challenges. Data Mesh treats domains as a first-class concern by distributing the data ownership from the central team to each data domain, while keeping the federated governance to monitor domains and their data products. Many multi-million dollar organizations like Paypal, Netflix, and Zalando have already transformed their data analysis pipelines based on this new architecture. In this decentralized architecture where data is locally preserved by each domain team, traditional centralized machine learning is incapable of conducting effective analysis across multiple domains, especially for security-sensitive organizations. To this end, we introduce a pioneering approach that incorporates Federated Learning into Data Mesh. To the best of our knowledge, this is the first open-source applied work that represents a critical advancement toward the integration of federated learning methods into the Data Mesh paradigm, underscoring the promising prospects for privacy-preserving and decentralized data analysis strategies within Data Mesh architecture.","Tue, 26 Mar 2024 17:10:15 UTC (929 KB)"
"53","To Supervise or Not to Supervise: Understanding and Addressing the Key Challenges of 3D Transfer Learning","Souhail Hadgi, Lei Li, Maks Ovsjanikov","Computer Vision and Pattern Recognition (cs.CV)","Transfer learning has long been a key factor in the advancement of many fields including 2D image analysis. Unfortunately, its applicability in 3D data processing has been relatively limited. While several approaches for 3D transfer learning have been proposed in recent literature, with contrastive learning gaining particular prominence, most existing methods in this domain have only been studied and evaluated in limited scenarios. Most importantly, there is currently a lack of principled understanding of both when and why 3D transfer learning methods are applicable. Remarkably, even the applicability of standard supervised pre-training is poorly understood. In this work, we conduct the first in-depth quantitative and qualitative investigation of supervised and contrastive pre-training strategies and their utility in downstream 3D tasks. We demonstrate that layer-wise analysis of learned features provides significant insight into the downstream utility of trained networks. Informed by this analysis, we propose a simple geometric regularization strategy, which improves the transferability of supervised pre-training. Our work thus sheds light onto both the specific challenges of 3D transfer learning, as well as strategies to overcome them.","Tue, 26 Mar 2024 16:57:33 UTC (5,797 KB)"
"54","Using Domain Knowledge to Guide Dialog Structure Induction via Neural Probabilistic Soft Logic","Connor Pryor, Quan Yuan, Jeremiah Liu, Mehran Kazemi, Deepak Ramachandran, Tania Bedrax-Weiss, Lise Getoor","Computation and Language (cs.CL)","Dialog Structure Induction (DSI) is the task of inferring the latent dialog structure (i.e., a set of dialog states and their temporal transitions) of a given goal-oriented dialog. It is a critical component for modern dialog system design and discourse analysis. Existing DSI approaches are often purely data-driven, deploy models that infer latent states without access to domain knowledge, underperform when the training corpus is limited/noisy, or have difficulty when test dialogs exhibit distributional shifts from the training domain. This work explores a neural-symbolic approach as a potential solution to these problems. We introduce Neural Probabilistic Soft Logic Dialogue Structure Induction (NEUPSL DSI), a principled approach that injects symbolic knowledge into the latent space of a generative neural model. We conduct a thorough empirical investigation on the effect of NEUPSL DSI learning on hidden representation quality, few-shot learning, and out-of-domain generalization performance. Over three dialog structure induction datasets and across unsupervised and semi-supervised settings for standard and cross-domain generalization, the injection of symbolic knowledge using NEUPSL DSI provides a consistent boost in performance over the canonical baselines.","Tue, 26 Mar 2024 16:42:30 UTC (3,380 KB)"
"55","Learning the Optimal Power Flow: Environment Design Matters","Thomas Wolgast, Astrid Nieße","Machine Learning (cs.LG)","To solve the optimal power flow (OPF) problem, reinforcement learning (RL) emerges as a promising new approach. However, the RL-OPF literature is strongly divided regarding the exact formulation of the OPF problem as an RL environment. In this work, we collect and implement diverse environment design decisions from the literature regarding training data, observation space, episode definition, and reward function choice. In an experimental analysis, we show the significant impact of these environment design options on RL-OPF training performance. Further, we derive some first recommendations regarding the choice of these design decisions. The created environment framework is fully open-source and can serve as a benchmark for future research in the RL-OPF field.","Tue, 26 Mar 2024 16:13:55 UTC (177 KB)"
"56","Band Structure and Fermi Surface Nesting in $LaSb_2$","Evan O'Leary, Lin-Lin Wang, Yevhen Kushnirenko, Ben Schrunk, Andrew Eaton, Paula Herrera-Siklody, Paul C. Canfield, Adam Kaminski","Strongly Correlated Electrons (cond-mat.str-el)","We use high-resolution angle resolved photoemission spectroscopy (ARPES) and density functional theory (DFT) to investigate the electronic structure of the charge density wave (CDW) system LaSb$_2$. This compound is among an interesting group of materials that manifests both a CDW transition and lower temperature superconductivity. We find the DFT calculations to be in good agreement with our ARPES data. The Fermi surface of LaSb$_2$ consists of two small hole pockets close to $\Gamma$ and four larger pockets near the Brillouin zone (BZ) boundary. The overall features of the Fermi surface do not vary with temperature. A saddle point is present at -0.19 $eV$ below the Fermi level at $\Gamma$. Critical points in band structure have more pronounced effects on a materials properties when they are located closer to the Fermi level, making doped LaSb$_2$ compounds a potential interesting subject of future research. Multiple peaks are present in the generalized, electronic susceptibility calculations indicating the presence of possible nesting vectors. We were not able to detect any signatures of the CDW transition at 355 K, pointing to the subtle nature of this transition. This is unusual, given that such a high transition temperature is expected to be associated with the presence of a large CDW gap. This is confirmed through investigation of the Fermi surface and through analysis of momentum distribution curves (MDC). It is possible that changes are subtle and occur below current sensitivity of our measurements.","Tue, 26 Mar 2024 16:06:07 UTC (9,603 KB)"
"57","Towards Multilevel Modelling of Train Passing Events on the Staffordshire Bridge","Lawrence A. Bull, Chiho Jeon, Mark Girolami, Andrew Duncan, Jennifer Schooling, Miguel Bravo Haro","Applications (stat.AP)","We suggest a multilevel model, to represent aggregate train-passing events from the Staffordshire bridge monitoring system. We formulate a combined model from simple units, representing strain envelopes (of each train passing) for two types of commuter train. The measurements are treated as a longitudinal dataset and represented with a (low-rank approximation) hierarchical Gaussian process. For each unit in the combined model, we encode domain expertise as boundary condition constraints and work towards a general representation of the strain response. Looking forward, this should allow for the simulation of train types that were previously unobserved in the training data. For example, trains with more passengers or freights with a heavier payload. The strain event simulations are valuable since they can inform further experiments (including FEM calibration, fatigue analysis, or design) to test the bridge in hypothesised scenarios.","Tue, 26 Mar 2024 15:55:54 UTC (4,823 KB)"
"58","CSSTs: A Dynamic Data Structure for Partial Orders in Concurrent Execution Analysis","Hünkar Can Tunç, Ameya Prashant Deshmukh, Berk Çirisci, Constantin Enea, Andreas Pavlogiannis","Programming Languages (cs.PL)","Dynamic analyses are a standard approach to analyzing and testing concurrent programs. Such techniques observe program traces and analyze them to infer the presence or absence of bugs. At its core, each analysis maintains a partial order $P$ that represents order dependencies between events of the analyzed trace $\sigma$. Naturally, the scalability of the analysis largely depends on how efficiently it maintains $P$. The standard data structure for this task has thus far been vector clocks. These, however, are slow for analyses that follow a non-streaming style, costing $O(n)$ for inserting (and propagating) each new ordering in $P$, where $n$ is the size of $\sigma$, while they cannot handle the deletion of existing orderings.
In this paper we develop collective sparse segment trees (CSSTs), a simple but elegant data structure for generically maintaining a partial order $P$. CSSTs thrive when the width $k$ of $P$ is much smaller than the size $n$ of its domain, allowing inserting, deleting, and querying for orderings in $P$ to run in $O(logn)$ time. For a concurrent trace, $k$ is bounded by the number of its threads, and is normally orders of magnitude smaller than its size $n$, making CSSTs fitting for this setting. Our experimental results confirm that CSSTs are the best data structure currently to handle a range of dynamic analyses from existing literature.","Tue, 26 Mar 2024 15:53:53 UTC (124 KB)"
"59","Probing the shape of the quark-gluon plasma droplet via event-by-event QGP tomography","Bithika Karmakar, Dusan Zigic, Pasi Huovinen, Marko Djordjevic, Magdalena Djordjevic, Jussi Auvinen","High Energy Physics - Phenomenology (hep-ph)","This study investigates Quark-Gluon Plasma (QGP) in heavy-ion collisions through two avenues: high-$p_{\perp}$ frameworks and hydrodynamic modeling. Using the T$_{\text{R}}$ENTo model, we find that IP-Glasma mimicking $p=0$ value aligns well with high-$p_{\perp}$ data, in agreement with Bayesian analysis of the low-$p_{\perp}$ regime. While adjusting $p$ values may improve a fit to a particular high-$p_{\perp}$ observable, it does not permit an earlier onset of transverse expansion.","Tue, 26 Mar 2024 15:53:07 UTC (884 KB)"
"60","Graph Language Model (GLM): A new graph-based approach to detect social instabilities","Wallyson Lemes de Oliveira, Vahid Shamsaddini, Ali Ghofrani, Rahul Singh Inda, Jithendra Sai Veeramaneni, Étienne Voutaz","Computation and Language (cs.CL)","This scientific report presents a novel methodology for the early prediction of important political events using News datasets. The methodology leverages natural language processing, graph theory, clique analysis, and semantic relationships to uncover hidden predictive signals within the data. Initially, we designed a preliminary version of the method and tested it on a few events. This analysis revealed limitations in the initial research phase. We then enhanced the model in two key ways: first, we added a filtration step to only consider politically relevant news before further processing; second, we adjusted the input features to make the alert system more sensitive to significant spikes in the data. After finalizing the improved methodology, we tested it on eleven events including US protests, the Ukraine war, and French protests. Results demonstrate the superiority of our approach compared to baseline methods. Through targeted refinements, our model can now provide earlier and more accurate predictions of major political events based on subtle patterns in news data.","Tue, 26 Mar 2024 15:53:02 UTC (15,022 KB)"
"61","Exploring the Boundaries of Ambient Awareness in Twitter","Pablo Sanchez-Martin, Sonja Utz, Isabel Valera","Social and Information Networks (cs.SI)","Ambient awareness refers to the ability of social media users to obtain knowledge about who knows what (i.e., users' expertise) in their network, by simply being exposed to other users' content (e.g, tweets on Twitter). Previous work, based on user surveys, reveals that individuals self-report ambient awareness only for parts of their networks. However, it is unclear whether it is their limited cognitive capacity or the limited exposure to diagnostic tweets (i.e., online content) that prevents people from developing ambient awareness for their complete network. In this work, we focus on in-wall ambient awareness (IWAA) in Twitter and conduct a two-step data-driven analysis, that allows us to explore to which extent IWAA is likely, or even possible. First, we rely on reactions (e.g., likes), as strong evidence of users being aware of experts in Twitter. Unfortunately, such strong evidence can be only measured for active users, which represent the minority in the network. Thus to study the boundaries of IWAA to a larger extent, in the second part of our analysis, we instead focus on the passive exposure to content generated by other users -- which we refer to as in-wall visibility. This analysis shows that (in line with \citet{levordashka2016ambient}) only for a subset of users IWAA is plausible, while for the majority it is unlikely, if even possible, to develop IWAA. We hope that our methodology paves the way for the emergence of data-driven approaches for the study of ambient awareness.","Tue, 26 Mar 2024 15:09:33 UTC (2,405 KB)"
"62","Noise2Noise Denoising of CRISM Hyperspectral Data","Robert Platt, Rossella Arcucci, Cédric M. John","Computer Vision and Pattern Recognition (cs.CV)","Hyperspectral data acquired by the Compact Reconnaissance Imaging Spectrometer for Mars (CRISM) have allowed for unparalleled mapping of the surface mineralogy of Mars. Due to sensor degradation over time, a significant portion of the recently acquired data is considered unusable. Here a new data-driven model architecture, Noise2Noise4Mars (N2N4M), is introduced to remove noise from CRISM images. Our model is self-supervised and does not require zero-noise target data, making it well suited for use in Planetary Science applications where high quality labelled data is scarce. We demonstrate its strong performance on synthetic-noise data and CRISM images, and its impact on downstream classification performance, outperforming benchmark methods on most metrics. This allows for detailed analysis for critical sites of interest on the Martian surface, including proposed lander sites.","Tue, 26 Mar 2024 14:49:22 UTC (3,683 KB)"
"63","Regularity for nonlocal equations with local Neumann boundary conditions","Xavier Ros-Oton, Marvin Weidner","Analysis of PDEs (math.AP)","In this article we establish fine results on the boundary behavior of solutions to nonlocal equations in $C^{k,\gamma}$ domains which satisfy local Neumann conditions on the boundary. Such solutions typically blow up at the boundary like $v \asymp d^{s-1}$ and are sometimes called large solutions. In this setup we prove optimal regularity results for the quotients $v/d^{s-1}$, depending on the regularity of the domain and on the data of the problem. The results of this article will be important in a forthcoming work on nonlocal free boundary problems.","Tue, 26 Mar 2024 14:11:19 UTC (46 KB)"
"64","Nuclear matrix elements of neutrinoless double-beta decay in covariant density functional theory with different mechanisms","C. R. Ding, Gang Li, J. M. Yao","Nuclear Theory (nucl-th)","Nuclear matrix elements (NMEs) for neutrinoless double-beta ($0\nu\beta\beta$) decay in candidate nuclei play a crucial role in interpreting results from current experiments and in designing future ones. Accurate NME values serve as important nuclear inputs for constraining parameters in new physics, such as neutrino mass and the Wilson coefficients of lepton-number-violating (LNV) operators. In this study, we present a comprehensive calculation of NMEs for $0\nu\beta\beta$ decay in $^{76}$Ge, $^{82}$Se, $^{100}$Mo, $^{130}$Te, and $^{136}$Xe, using nuclear wave functions obtained from multi-reference covariant density functional theory (MR-CDFT). We employ three types of transition potentials at the leading order in chiral effective field theory. Our results, along with recent data, are utilized to constrain the coefficients of LNV operators. We find that NMEs for the standard and short-range mechanisms are significantly larger than those for the non-standard long-range mechanism. The use of NMEs from various nuclear models does not notably change the parameter space intervals for the coefficients, although MR-CDFT yields the most stringent constraint. Furthermore, our NMEs can also be used to perform a more comprehensive analysis with multiple isotopes.","Tue, 26 Mar 2024 14:06:23 UTC (2,985 KB)"
"65","Low-temperature benchmarking of qubit control wires by primary electron thermometry","Elias Roos Hansen, Ferdinand Kuemmeth, Joost van der Heijden","Mesoscale and Nanoscale Physics (cond-mat.mes-hall)","Low-frequency qubit control wires require non-trivial thermal anchoring and low-pass filtering. The resulting electron temperature serves as a quality benchmark for these signal lines. In this technical note, we make use of a primary electron thermometry technique, using a Coulomb blockade thermometer, to establish the electron temperature in the millikelvin regime. The experimental four-probe measurement setup, the data analysis, and the measurement limitations are discussed in detail. We verify the results by also using another electron thermometry technique, based on a superconductor-insulator-normal metal junction. Our comparison of signal lines with QDevil's QFilter to unfiltered signal lines demonstrates that the filter significantly reduces both the rms noise and electron temperature, which is measured to be 22 $\pm$ 1 mK.","Tue, 26 Mar 2024 14:03:19 UTC (8,966 KB)"
"66","Panonut360: A Head and Eye Tracking Dataset for Panoramic Video","Yutong Xu, Junhao Du, Jiahe Wang, Yuwei Ning, Sihan Zhou Yang Cao","Computer Vision and Pattern Recognition (cs.CV)","With the rapid development and widespread application of VR/AR technology, maximizing the quality of immersive panoramic video services that match users' personal preferences and habits has become a long-standing challenge. Understanding the saliency region where users focus, based on data collected with HMDs, can promote multimedia encoding, transmission, and quality assessment. At the same time, large-scale datasets are essential for researchers and developers to explore short/long-term user behavior patterns and train AI models related to panoramic videos. However, existing panoramic video datasets often include low-frequency user head or eye movement data through short-term videos only, lacking sufficient data for analyzing users' Field of View (FoV) and generating video saliency regions.
Driven by these practical factors, in this paper, we present a head and eye tracking dataset involving 50 users (25 males and 25 females) watching 15 panoramic videos. The dataset provides details on the viewport and gaze attention locations of users. Besides, we present some statistics samples extracted from the dataset. For example, the deviation between head and eye movements challenges the widely held assumption that gaze attention decreases from the center of the FoV following a Gaussian distribution. Our analysis reveals a consistent downward offset in gaze fixations relative to the FoV in experimental settings involving multiple users and videos. That's why we name the dataset Panonut, a saliency weighting shaped like a donut. Finally, we also provide a script that generates saliency distributions based on given head or eye coordinates and pre-generated saliency distribution map sets of each video from the collected eye tracking data.
The dataset is available on website: this https URL.","Tue, 26 Mar 2024 13:54:52 UTC (11,373 KB)"
"67","Not All Similarities Are Created Equal: Leveraging Data-Driven Biases to Inform GenAI Copyright Disputes","Uri Hacohen, Adi Haviv, Shahar Sarfaty, Bruria Friedman, Niva Elkin-Koren, Roi Livni, Amit H Bermano","Computer Vision and Pattern Recognition (cs.CV)","The advent of Generative Artificial Intelligence (GenAI) models, including GitHub Copilot, OpenAI GPT, and Stable Diffusion, has revolutionized content creation, enabling non-professionals to produce high-quality content across various domains. This transformative technology has led to a surge of synthetic content and sparked legal disputes over copyright infringement. To address these challenges, this paper introduces a novel approach that leverages the learning capacity of GenAI models for copyright legal analysis, demonstrated with GPT2 and Stable Diffusion models. Copyright law distinguishes between original expressions and generic ones (Scènes à faire), protecting the former and permitting reproduction of the latter. However, this distinction has historically been challenging to make consistently, leading to over-protection of copyrighted works. GenAI offers an unprecedented opportunity to enhance this legal analysis by revealing shared patterns in preexisting works. We propose a data-driven approach to identify the genericity of works created by GenAI, employing ""data-driven bias"" to assess the genericity of expressive compositions. This approach aids in copyright scope determination by utilizing the capabilities of GenAI to identify and prioritize expressive elements and rank them according to their frequency in the model's dataset. The potential implications of measuring expressive genericity for copyright law are profound. Such scoring could assist courts in determining copyright scope during litigation, inform the registration practices of Copyright Offices, allowing registration of only highly original synthetic works, and help copyright owners signal the value of their works and facilitate fairer licensing deals. More generally, this approach offers valuable insights to policymakers grappling with adapting copyright law to the challenges posed by the era of GenAI.","Tue, 26 Mar 2024 13:32:32 UTC (10,819 KB)"
"68","Assessing the similarity of real matrices with arbitrary shape","Jasper Albers, Anno C. Kurth, Robin Gutzen, Aitor Morales-Gregorio, Michael Denker, Sonja Grün, Sacha J. van Albada, Markus Diesmann","Neurons and Cognition (q-bio.NC)","Assessing the similarity of matrices is valuable for analyzing the extent to which data sets exhibit common features in tasks such as data clustering, dimensionality reduction, pattern recognition, group comparison, and graph analysis. Methods proposed for comparing vectors, such as cosine similarity, can be readily generalized to matrices. However, this approach usually neglects the inherent two-dimensional structure of matrices. Here, we propose singular angle similarity (SAS), a measure for evaluating the structural similarity between two arbitrary, real matrices of the same shape based on singular value decomposition. After introducing the measure, we compare SAS with standard measures for matrix comparison and show that only SAS captures the two-dimensional structure of matrices. Further, we characterize the behavior of SAS in the presence of noise and as a function of matrix dimensionality. Finally, we apply SAS to two use cases: square non-symmetric matrices of probabilistic network connectivity, and non-square matrices representing neural brain activity. For synthetic data of network connectivity, SAS matches intuitive expectations and allows for a robust assessment of similarities and differences. For experimental data of brain activity, SAS captures differences in the structure of high-dimensional responses to different stimuli. We conclude that SAS is a suitable measure for quantifying the shared structure of matrices with arbitrary shape.","Tue, 26 Mar 2024 13:24:52 UTC (1,141 KB)"
"69","Analysis on reservoir activation with the nonlinearity harnessed from solution-processed MoS2 devices","Songwei Liu, Yang Liu, Yingyi Wen, Jingfang Pei, Pengyu Liu, Lekai Song, Xiaoyue Fan, Wenchen Yang, Danmei Pan, Teng Ma, Yue Lin, Gang Wang, Guohua Hu","Applied Physics (physics.app-ph)","Reservoir computing is a recurrent neural network that has been applied across various domains in machine learning. The implementation of reservoir computing, however, often demands heavy computations for activating the reservoir. Configuring physical reservoir networks and harnessing the nonlinearity from the underlying devices for activation is an emergent solution to address the computational challenge. Herein, we analyze the feasibility of employing the nonlinearity from solution-processed molybdenum disulfide (MoS2) devices for reservoir activation. The devices, fabricated using liquid-phase exfoliated MoS2, exhibit a high-order nonlinearity achieved by Stark modulation of the MoS2 material. We demonstrate that this nonlinearity can be fitted and employed as the activation function to facilitate reservoir computing implementation. Notably, owing to the high-order nonlinearity, the network exhibits long-term synchronization and robust generalization abilities for approximating complex dynamical systems. Given the remarkable reservoir activation capability, coupled with the scalability of the device fabrication, our findings open the possibility for the physical realization of lightweight, efficient reservoir computing for, for instance, signal classification, motion tracking, and pattern recognition of complex time series as well as secure cryptography. As an example, we show the network can be appointed to generate chaotic random numbers for secure data encryption.","Tue, 26 Mar 2024 13:04:00 UTC (2,609 KB)"
"70","How Private is DP-SGD?","Lynn Chua, Badih Ghazi, Pritish Kamath, Ravi Kumar, Pasin Manurangsi, Amer Sinha, Chiyuan Zhang","Machine Learning (cs.LG)","We demonstrate a substantial gap between the privacy guarantees of the Adaptive Batch Linear Queries (ABLQ) mechanism under different types of batch sampling: (i) Shuffling, and (ii) Poisson subsampling; the typical analysis of Differentially Private Stochastic Gradient Descent (DP-SGD) follows by interpreting it as a post-processing of ABLQ. While shuffling based DP-SGD is more commonly used in practical implementations, it is neither analytically nor numerically amenable to easy privacy analysis. On the other hand, Poisson subsampling based DP-SGD is challenging to scalably implement, but has a well-understood privacy analysis, with multiple open-source numerically tight privacy accountants available. This has led to a common practice of using shuffling based DP-SGD in practice, but using the privacy analysis for the corresponding Poisson subsampling version. Our result shows that there can be a substantial gap between the privacy analysis when using the two types of batch sampling, and thus advises caution in reporting privacy parameters for DP-SGD.","Tue, 26 Mar 2024 13:02:43 UTC (684 KB)"
"71","Constraining Primordial Non-Gaussianity from Large Scale Structure with the Wavelet Scattering Transform","Matteo Peron, Gabriel Jung, Michele Liguori, Massimo Pietroni","Cosmology and Nongalactic Astrophysics (astro-ph.CO)","We investigate the Wavelet Scattering Transform (WST) as a tool for the study of Primordial non-Gaussianity (PNG) in Large Scale Structure (LSS), and compare its performance with that achievable via a joint analysis with power spectrum and bispectrum (P+B). We consider the three main primordial bispectrum shapes - local, equilateral and orthogonal - and produce Fisher forecast for the corresponding fNL amplitude parameters, jointly with standard cosmological parameters. We analyze simulations from the publicly available ""Quijote"" and ""Quijote-png"" N-body suites, studying both the dark matter and halo fields. We find that the WST outperforms the power spectrum alone on all parameters, both on the fNL's and on cosmological ones. In particular, on fNL_loc for halos, the improvement is about 27%. When B is combined with P, halo constraints from WST are weaker for fNL_loc (at ~ 15% level), but stronger for fNL_eq (~ 25%) and fNL_ortho (~ 28%). Our results show that WST, both alone and in combination with P+B, can improve the extraction of information on PNG from LSS data over the one attainable by a standard P+B analysis. Moreover, we identify a class of WST in which the origin of the extra information on PNG can be cleanly isolated.","Tue, 26 Mar 2024 12:40:49 UTC (18,902 KB)"
"72","Microscale Morphology Driven Thermal Transport in Fiber Reinforced Polymer Composites","Sabarinathan P Subramaniyan, Jonathan D Boehnlein, Pavana Prabhakar","Applied Physics (physics.app-ph)","Fiber-reinforced polymer composite (FRPC) materials are used extensively in various industries, such as aerospace, automobiles, and electronics packaging, due to their remarkable specific strength and desirable properties, such as enhanced durability and corrosion resistance. The evolution of thermal properties in FRPCs is crucial for advancing thermal management systems, optimizing material performance, and enhancing energy efficiency across these diverse sectors. Despite significant research efforts to develop new materials with improved thermal properties and reduced thermal degradation, there is a lack of understanding of the thermal transport phenomena considering the influence of microscale reinforcement morphology in these composites. In the current study, we performed experimental investigations complemented by computations to determine the thermal transport properties and associated phenomena in epoxy and carbon fiber-reinforced epoxy composites. The experimental findings were utilized as input data for numerical analysis to examine the impact of fiber morphology and volume fraction in thermal transport phenomena. Our results revealed that composites incorporating non-circular fibers manifested higher thermal conductivity than traditional circular fibers in the transverse direction. This can be attributed to increased interconnected heat flow pathways facilitated by the increased surface area of non-circular fibers with the same cross-sectional areas, resulting in efficient heat transfer.","Tue, 26 Mar 2024 12:31:33 UTC (1,801 KB)"
"73","Data-driven Energy Consumption Modelling for Electric Micromobility using an Open Dataset","Yue Ding, Sen Yan, Maqsood Hussain Shah, Hongyuan Fang, Ji Li, Mingming Liu","Artificial Intelligence (cs.AI)","The escalating challenges of traffic congestion and environmental degradation underscore the critical importance of embracing E-Mobility solutions in urban spaces. In particular, micro E-Mobility tools such as E-scooters and E-bikes, play a pivotal role in this transition, offering sustainable alternatives for urban commuters. However, the energy consumption patterns for these tools are a critical aspect that impacts their effectiveness in real-world scenarios and is essential for trip planning and boosting user confidence in using these. To this effect, recent studies have utilised physical models customised for specific mobility tools and conditions, but these models struggle with generalization and effectiveness in real-world scenarios due to a notable absence of open datasets for thorough model evaluation and verification. To fill this gap, our work presents an open dataset, collected in Dublin, Ireland, specifically designed for energy modelling research related to E-Scooters and E-Bikes. Furthermore, we provide a comprehensive analysis of energy consumption modelling based on the dataset using a set of representative machine learning algorithms and compare their performance against the contemporary mathematical models as a baseline. Our results demonstrate a notable advantage for data-driven models in comparison to the corresponding mathematical models for estimating energy consumption. Specifically, data-driven models outperform physical models in accuracy by up to 83.83% for E-Bikes and 82.16% for E-Scooters based on an in-depth analysis of the dataset under certain assumptions.","Tue, 26 Mar 2024 12:08:05 UTC (23,196 KB)"
"74","Online Tree Reconstruction and Forest Inventory on a Mobile Robotic System","Leonard Freißmuth, Matias Mattamala, Nived Chebrolu, Simon Schaefer, Stefan Leutenegger, Maurice Fallon","Robotics (cs.RO)","Terrestrial laser scanning (TLS) is the standard technique used to create accurate point clouds for digital forest inventories. However, the measurement process is demanding, requiring up to two days per hectare for data collection, significant data storage, as well as resource-heavy post-processing of 3D data. In this work, we present a real-time mapping and analysis system that enables online generation of forest inventories using mobile laser scanners that can be mounted e.g. on mobile robots. Given incrementally created and locally accurate submaps-data payloads-our approach extracts tree candidates using a custom, Voronoi-inspired clustering algorithm. Tree candidates are reconstructed using an adapted Hough algorithm, which enables robust modeling of the tree stem. Further, we explicitly incorporate the incremental nature of the data collection by consistently updating the database using a pose graph LiDAR SLAM system. This enables us to refine our estimates of the tree traits if an area is revisited later during a mission. We demonstrate competitive accuracy to TLS or manual measurements using laser scanners that we mounted on backpacks or mobile robots operating in conifer, broad-leaf and mixed forests. Our results achieve RMSE of 1.93 cm, a bias of 0.65 cm and a standard deviation of 1.81 cm (averaged across these sequences)-with no post-processing required after the mission is complete.","Tue, 26 Mar 2024 11:51:58 UTC (5,709 KB)"
"75","Lattice dynamics of LiNb$_{\text{1-x}}$Ta$_{\text{x}}$O$_{\text{3}}$ solid solutions: Theory and experiment","Felix Bernhardt, Soham Gharat, Alexander Kapp, Florian Pfeiffer, Robin Buschbeck, Franz Hempel, Oleksiy Pashkin, Susanne C. Kehr, Michael Rüsing, Simone Sanna, Lukas M. Eng","Materials Science (cond-mat.mtrl-sci)","Lithium niobate (LNO) and lithium tantalate (LTO) see widespread use in fundamental research and commercial technologies reaching from electronics over classical optics to integrated quantum communication. In recent years, the mixed crystal system lithium niobate tantalate (LNT) allows for the dedicate engineering of material properties by combining the advantages of the two parental materials LNO and LTO. Vibrational spectroscopies such as Raman spectroscopy or (Fourier transform) infrared spectroscopy are vital techniques to provide detailed insight into the material properties, which is central to the analysis and optimization of devices. In this work, we present a joint experimental-theoretical approach allowing to unambiguously assign the spectral features in the LNT material family through both Raman and IR spectroscopy, as well as to provide an in-depth explanation for the observed scattering efficiencies based on first-principles calculations. The phononic contribution to the static dielectric tensor is calculated from the experimental and theoretical data using the generalized Lyddane-Sachs-Teller relation and compared with the results of the first-principles calculations. The joint methodology can be readily expanded to other materials and serves, e.g., as the basis for studying the role of point defects or doping.","Tue, 26 Mar 2024 11:05:32 UTC (2,245 KB)"
"76","Parameterized Analysis of Bribery in Challenge the Champ Tournaments","Juhi Chaudhary, Hendrik Molter, Meirav Zehavi","Data Structures and Algorithms (cs.DS)","Challenge the champ tournaments are one of the simplest forms of competition, where a (initially selected) champ is repeatedly challenged by other players. If a player beats the champ, then that player is considered the new (current) champ. Each player in the competition challenges the current champ once in a fixed order. The champ of the last round is considered the winner of the tournament. We investigate a setting where players can be bribed to lower their winning probability against the initial champ. The goal is to maximize the probability of the initial champ winning the tournament by bribing the other players, while not exceeding a given budget for the bribes. Mattei et al. [Journal of Applied Logic, 2015] showed that the problem can be solved in pseudo-polynomial time, and that it is in XP when parameterized by the number of players.
We show that the problem is weakly NP-hard and W[1]-hard when parameterized by the number of players. On the algorithmic side, we show that the problem is fixed-parameter tractable when parameterized either by the number of different bribe values or the number of different probability values. To this end, we establish several results that are of independent interest. In particular, we show that the product knapsack problem is W[1]-hard when parameterized by the number of items in the knapsack, and that constructive bribery for cup tournaments is W[1]-hard when parameterized by the number of players. Furthermore, we present a novel way of designing mixed integer linear programs, ensuring optimal solutions where all variables are integers.","Tue, 26 Mar 2024 10:53:25 UTC (60 KB)"
"77","A Survey on Deep Learning and State-of-the-arts Applications","Mohd Halim Mohd Noor, Ayokunle Olalekan Ige","Machine Learning (cs.LG)","Deep learning, a branch of artificial intelligence, is a computational model that uses multiple layers of interconnected units (neurons) to learn intricate patterns and representations directly from raw input data. Empowered by this learning capability, it has become a powerful tool for solving complex problems and is the core driver of many groundbreaking technologies and innovations. Building a deep learning model is a challenging task due to the algorithm`s complexity and the dynamic nature of real-world problems. Several studies have reviewed deep learning concepts and applications. However, the studies mostly focused on the types of deep learning models and convolutional neural network architectures, offering limited coverage of the state-of-the-art of deep learning models and their applications in solving complex problems across different domains. Therefore, motivated by the limitations, this study aims to comprehensively review the state-of-the-art deep learning models in computer vision, natural language processing, time series analysis and pervasive computing. We highlight the key features of the models and their effectiveness in solving the problems within each domain. Furthermore, this study presents the fundamentals of deep learning, various deep learning model types and prominent convolutional neural network architectures. Finally, challenges and future directions in deep learning research are discussed to offer a broader perspective for future researchers.","Tue, 26 Mar 2024 10:10:53 UTC (1,990 KB)"
"78","Practical Applications of Advanced Cloud Services and Generative AI Systems in Medical Image Analysis","Jingyu Xu, Binbin Wu, Jiaxin Huang, Yulu Gong, Yifan Zhang, Bo Liu","Artificial Intelligence (cs.AI)","The medical field is one of the important fields in the application of artificial intelligence technology. With the explosive growth and diversification of medical data, as well as the continuous improvement of medical needs and challenges, artificial intelligence technology is playing an increasingly important role in the medical field. Artificial intelligence technologies represented by computer vision, natural language processing, and machine learning have been widely penetrated into diverse scenarios such as medical imaging, health management, medical information, and drug research and development, and have become an important driving force for improving the level and quality of medical services.The article explores the transformative potential of generative AI in medical imaging, emphasizing its ability to generate syntheticACM-2 data, enhance images, aid in anomaly detection, and facilitate image-to-image translation. Despite challenges like model complexity, the applications of generative models in healthcare, including Med-PaLM 2 technology, show promising results. By addressing limitations in dataset size and diversity, these models contribute to more accurate diagnoses and improved patient outcomes. However, ethical considerations and collaboration among stakeholders are essential for responsible implementation. Through experiments leveraging GANs to augment brain tumor MRI datasets, the study demonstrates how generative AI can enhance image quality and diversity, ultimately advancing medical diagnostics and patient care.","Tue, 26 Mar 2024 09:55:49 UTC (503 KB)"
"79","Determination of nuclear matter radii by means of microscopic optical potentials: the case of $^{78}$Kr","Matteo Vorabbi, Paolo Finelli, Carlotta Giusti","Nuclear Theory (nucl-th)","In this work we use microscopic Nucleon-Nucleus Optical Potentials (OP) to analyze elastic scattering data for the differential cross section of the $^{78}$Kr (p,p) $^{78}$Kr reaction, with the goal of extracting the matter radius and estimating the neutron skin, quantities that are both needed to determine the slope parameter $L$ of the nuclear symmetry energy. Our analysis is performed with the factorized version of the microscopic OP obtained in a previous series of papers within the Watson multiple scattering theory at the first order of the spectator expansion, which is based on the underlying nucleon-nucleon dynamics and is free from phenomenological inputs. Differently from our previous applications, the proton and neutron densities are described with a two-parameter Fermi (2pF) distribution, which makes the extraction of the matter radius easier and allows us to make a meaningful comparison with the original analysis, that was performed with the Glauber model. With standard minimization techniques we performed data analysis and extracted the matter radius and the neutron skin. Our analysis produces a matter radius of $R_m^{\rm (rms)} = 4.12$ fm, in good agreement with previous matter radii extracted from $^{76}$Kr and $^{80}$Kr, and a neutron skin of $\Delta R_{np} \simeq - 0.1$ fm, compatible with a previous analysis. Our factorized microscopic OP, supplied with 2pF densities, is a valuable tool to perform the analysis of the experimental differential cross section and extract information such as matter radius and neutron skin. Without any free parameters it provides a reasonably good description of the experimental differential cross section for scattering angles up to $\approx$ 40 degrees. Compared to the Glauber model our OP can be applied to a wider range of scattering angles and allows one to probe the nuclear systems in a more internal region.","Tue, 26 Mar 2024 09:46:56 UTC (105 KB)"
"80","Prediction-sharing During Training and Inference","Yotam Gafni, Ronen Gradwohl, Moshe Tennenholtz","Theoretical Economics (econ.TH)","Two firms are engaged in a competitive prediction task. Each firm has two sources of data -- labeled historical data and unlabeled inference-time data -- and uses the former to derive a prediction model, and the latter to make predictions on new instances. We study data-sharing contracts between the firms. The novelty of our study is to introduce and highlight the differences between contracts that share prediction models only, contracts to share inference-time predictions only, and contracts to share both. Our analysis proceeds on three levels. First, we develop a general Bayesian framework that facilitates our study. Second, we narrow our focus to two natural settings within this framework: (i) a setting in which the accuracy of each firm's prediction model is common knowledge, but the correlation between the respective models is unknown; and (ii) a setting in which two hypotheses exist regarding the optimal predictor, and one of the firms has a structural advantage in deducing it. Within these two settings we study optimal contract choice. More specifically, we find the individually rational and Pareto-optimal contracts for some notable cases, and describe specific settings where each of the different sharing contracts emerge as optimal. Finally, in the third level of our analysis we demonstrate the applicability of our concepts in a synthetic simulation using real loan data.","Tue, 26 Mar 2024 09:18:50 UTC (1,011 KB)"
"81","baskexact: An R package for analytical calculation of basket trial operating characteristics","Lukas Baumann","Computation (stat.CO)","Basket trials are a new type of clinical trial in which a treatment is investigated in several subgroups. For the analysis of these trials, information is shared between the subgroups based on the observed data to increase the power. Many approaches for the analysis of basket trials have been suggested, but only a few have been implemented in open source software packages. The R package baskexact facilitates the evaluation of two basket trial designs which use empirical Bayes techniques for sharing information. With baskexact, operating characteristics for single-stage and two-stage designs can be calculated analytically and optimal tuning parameters can be selected.","Tue, 26 Mar 2024 09:11:58 UTC (240 KB)"
"82","Algorithmic unfolding for image reconstruction and localization problems in fluorescence microscopy","Silvia Bonettini, Luca Calatroni, Danilo Pezzi, Marco Prato","Numerical Analysis (math.NA)","We propose an unfolded accelerated projected-gradient descent procedure to estimate model and algorithmic parameters for image super-resolution and molecule localization problems in image microscopy. The variational lower-level constraint enforces sparsity of the solution and encodes different noise statistics (Gaussian, Poisson), while the upper-level cost assesses optimality w.r.t.~the task considered. In more detail, a standard $\ell_2$ cost is considered for image reconstruction (e.g., deconvolution/super-resolution, semi-blind deconvolution) problems, while a smoothed $\ell_1$ is employed to assess localization precision in some exemplary fluorescence microscopy problems exploiting single-molecule activation. Several numerical experiments are reported to validate the proposed approach on synthetic and realistic ISBI data.","Tue, 26 Mar 2024 09:08:25 UTC (3,044 KB)"
"83","A Type of Nonlinear Fréchet Regressions","Lu Lin, Ze Chen","Methodology (stat.ME)","The existing Fréchet regression is actually defined within a linear framework, since the weight function in the Fréchet objective function is linearly defined, and the resulting Fréchet regression function is identified to be a linear model when the random object belongs to a Hilbert space. Even for nonparametric and semiparametric Fréchet regressions, which are usually nonlinear, the existing methods handle them by local linear (or local polynomial) technique, and the resulting Fréchet regressions are (locally) linear as well. We in this paper introduce a type of nonlinear Fréchet regressions. Such a framework can be utilized to fit the essentially nonlinear models in a general metric space and uniquely identify the nonlinear structure in a Hilbert space. Particularly, its generalized linear form can return to the standard linear Fréchet regression through a special choice of the weight function. Moreover, the generalized linear form possesses methodological and computational simplicity because the Euclidean variable and the metric space element are completely separable. The favorable theoretical properties (e.g. the estimation consistency and presentation theorem) of the nonlinear Fréchet regressions are established systemically. The comprehensive simulation studies and a human mortality data analysis demonstrate that the new strategy is significantly better than the competitors.","Tue, 26 Mar 2024 08:23:37 UTC (66 KB)"
"84","Investigations on Physics-Informed Neural Networks for Aerodynamics","Guillaume Coulaud (ACUMES), Maxime Le (ACUMES), Régis Duvigneau (ACUMES)","Analysis of PDEs (math.AP)","Physics-Informed Neural Networks (PINNs) have recently emerged as a novel approach to simulate complex physical systems on the basis of both data observations and physical models. In this work, we investigate the use of PINNs for various applications in aerodynamics and we explain how to leverage their specific formulation to perform some tasks effectively. In particular, we demonstrate the ability of PINNs to construct parametric surrogate models, to achieve multiphysic couplings and to infer turbulence characteristics via data assimilation. The robustness and accuracy of the PINNs approach are analysed, then current issues and challenges are discussed.","Tue, 26 Mar 2024 07:57:56 UTC (1,234 KB)"
"85","Green HPC: An analysis of the domain based on Top500","Abdessalam Benhari (LIG, DATAMOVE ), Denis Trystram (UGA, DATAMOVE ), Fanny Dufossé (DATAMOVE), Yves Denneulin, Frédéric Desprez","Computers and Society (cs.CY)","The demand in computing power has never stopped growing over the years. Today, the performance of the most powerful systems exceeds the exascale and the number of petascale systems continues to grow. Unfortunately, this growth also goes hand in hand with ever-increasing energy costs, which in turn means a significant carbon footprint. In view of the environmental crisis, this paper intents to look at the often hidden issue of energy consumption of HPC systems. As it is not easy to access the data of the constructors, we then consider the Top500 as the tip of the iceberg to identify the trends of the whole domain.The objective of this work is to analyze Top500 and Green500 data from several perspectives in order to identify the dynamic of the domain regarding its environmental impact. The contributions are to take stock of the empirical laws governing the evolution of HPC computing systems both from the performance and energy perspectives, to analyze the most relevant data for developing the performance and energy efficiency of large-scale computing systems, to put these analyses into perspective with effects and impacts (lifespan of the HPC systems) and finally to derive a predictive model for the weight of HPC sector within the horizon 2030.","Tue, 26 Mar 2024 07:55:40 UTC (1,105 KB)"
"86","Localized Inverse Design in Conservation Laws and Hamilton-Jacobi Equations","Rinaldo M. Colombo, Vincent Perrollaz (IDP)","Analysis of PDEs (math.AP)","Consider the inverse design problem for a scalar conservation law, i.e., the problem of finding initial data evolving into a given profile at a given time. The solution we present below takes into account localizations both in the final interval where the profile is assigned and in the initial interval where the datum is sought, as well as additional a priori constraints on the datum's range provided by the model. These results are motivated and can be applied to data assimilation procedures in traffic modeling and accidents localization.","Tue, 26 Mar 2024 07:53:28 UTC (21 KB)"
"87","Rapid neutron star equation of state inference with Normalising Flows","Jordan McGinn, Arunava Mukherjee, Jessica Irwin, Christopher Messenger, Michael J. Williams, Ik Siong Heng","General Relativity and Quantum Cosmology (gr-qc)","The first direct detection of gravitational waves from binary neutron stars on the 17th of August, 2017, (GW170817) heralded the arrival of a new messenger for probing neutron star astrophysics and provided the first constraints on neutron star equation of state from gravitational wave observations. Significant computational effort was expended to obtain these first results and therefore, as observations of binary neutron star coalescence become more routine in the coming observing runs, there is a need to improve the analysis speed and flexibility. Here, we present a rapid approach for inferring the neutron star equation of state based on Normalising Flows. As a demonstration, using the same input data, our approach, ASTREOS, produces results consistent with those presented by the LIGO-Virgo collaboration but requires < 1 sec to generate neutron star equation of state confidence intervals. Furthermore, ASTREOS allows for non-parametric equation of state inference. This rapid analysis will not only facilitate neutron star equation of state studies but can potentially enhance future alerts for electromagnetic follow-up observations of binary neutron star mergers.","Tue, 26 Mar 2024 07:53:21 UTC (2,333 KB)"
"88","Study on high-frequency quasi-periodic oscillations in rotating black bounce spacetime","Jianbo Lu, Shining Yang, Mou Xu, Liu Yang","General Relativity and Quantum Cosmology (gr-qc)","We investigate dynamical effects of particles moving around rotating SV regular BH and traversable wormholes, and the deviation from Kerr BH. We found that the rotating SV traversable wormhole spacetime has a closer innermost stable circular orbit to the central object than Kerr and rotating regular BHs. Furthermore, the paper provides the general form of the epicycle frequency in this spacetime, presenting the radial profile of the angular frequency of particles undergoing oscillatory motion. Unlike the commonly used $\chi$-square analysis method, with considering a generalized data analysis approach in this paper we hypothesize that the observational data of three microguasars can be explained through an axisymmetric SV spacetime, fitting the data to find the range of spin values for three types of SV objects, and analyzing the possible mechanisms of HFQPOs (high-frequency quasi-periodic oscillations) generation for different types of axisymmetric SV objects. We found that for a Kerr BH with $l^*=0$, the observational data of three microquasars can be explained by the HFQPOs model used in this paper (excluding ER4 model); for a regular BH with $l^*=1$, the HFQPO phenomenon observed through the three microquasars can be explained by the ER0, ER1, ER2, ER3, RP0, RP1, RP2, TD and WD models. For a traversable wormhole with $l^*=3$, the mechanism of HFQPO can be interpreted by the ER1, ER3, RP0, RP1, RP2, TD and WD models. Moreover, we observed that for the same HFQPOs model, the limiting range of the spin parameter $\alpha^*$ increases with the increase of the parameter $l^*$.","Tue, 26 Mar 2024 07:40:47 UTC (837 KB)"
"89","Cost-benefit analysis of ecosystem modelling to support fisheries management","Matthew H. Holden, Eva E. Plagányi, Elizabeth A. Fulton, Alexander B. Campbell, Rachel Janes, Robyn A. Lovett, Montana Wickens, Matthew P. Adams, Larissa Lubiana Botelho, Catherine M. Dichmont, Philip Erm, Kate J Helmstedt, Ryan F. Heneghan, Manuela Mendiolar, Anthony J. Richardson, Jacob G. D. Rogers, Kate Saunders, Liam Timms","Populations and Evolution (q-bio.PE)","Mathematical and statistical models underlie many of the world's most important fisheries management decisions. Since the 19th century, difficulty calibrating and fitting such models has been used to justify the selection of simple, stationary, single-species models to aid tactical fisheries management decisions. Whereas these justifications are reasonable, it is imperative that we quantify the value of different levels of model complexity for supporting fisheries management, especially given a changing climate, where old methodologies may no longer perform as well as in the past. Here we argue that cost-benefit analysis is an ideal lens to assess the value of model complexity in fisheries management. While some studies have reported the benefits of model complexity in fisheries, modeling costs are rarely considered. In the absence of cost data in the literature, we report, as a starting point, relative costs of single-species stock assessment and marine ecosystem models from two Australian organizations. We found that costs varied by two orders of magnitude, and that ecosystem model costs increased with model complexity. Using these costs, we walk through a hypothetical example of cost-benefit analysis. The demonstration is intended to catalyze the reporting of modeling costs and benefits.","Tue, 26 Mar 2024 07:24:28 UTC (474 KB)"
"90","Particle identification with machine learning from incomplete data in the ALICE experiment","Maja Karwowska, Łukasz Graczykowski, Kamil Deja, Miłosz Kasak, Małgorzata Janik (for the ALICE collaboration)","High Energy Physics - Experiment (hep-ex)","The ALICE experiment at the LHC measures properties of the strongly interacting matter formed in ultrarelativistic heavy-ion collisions. Such studies require accurate particle identification (PID). ALICE provides PID information via several detectors for particles with momentum from about 100 MeV/c up to 20 GeV/c. Traditionally, particles are selected with rectangular cuts. Acmuch better performance can be achieved with machine learning (ML) methods. Our solution uses multiple neural networks (NN) serving as binary classifiers. Moreover, we extended our particle classifier with Feature Set Embedding and attention in order to train on data with incomplete samples. We also present the integration of the ML project with the ALICE analysis software, and we discuss domain adaptation, the ML technique needed to transfer the knowledge between simulated and real experimental data.","Tue, 26 Mar 2024 07:05:06 UTC (1,608 KB)"
"91","Coupling-Constant Averaged Exchange-Correlation Hole for He, Li, Be, N, Ne Atoms from CCSD","Lin Hou, Tom J. P. Irons, Yanyong Wang, James W. Furness, Andrew M. Wibowo-Teale, Jianwei Sun","Chemical Physics (physics.chem-ph)","Accurate approximation of the exchange-correlation (XC) energy in density functional theory (DFT) calculations is essential for reliably modelling electronic systems. Many such approximations are developed from models of the XC hole; accurate reference XC holes for real electronic systems are crucial for evaluating the accuracy of these models however the availability of reliable reference data is limited to a few systems. In this study, we employ the Lieb optimization with a coupled cluster singles and doubles (CCSD) reference to construct accurate coupling-constant averaged XC holes, resolved into individual exchange and correlation components, for five spherically symmetric atoms: He, Li, Be, N, and Ne. Alongside providing a new set of reference data for the construction and evaluation of model XC holes, we compare our data against the exchange and correlation hole models of the established LDA and PBE density functional approximations. Our analysis confirms the established rationalization for the limitations of LDA and the improvement observed with PBE in terms of the hole depth and its long-range decay, demonstrated in real-space for the series of spherically-symmetric atoms.","Tue, 26 Mar 2024 06:42:09 UTC (535 KB)"
"92","Neural Clustering based Visual Representation Learning","Guikun Chen, Xia Li, Yi Yang, Wenguan Wang","Computer Vision and Pattern Recognition (cs.CV)","We investigate a fundamental aspect of machine vision: the measurement of features, by revisiting clustering, one of the most classic approaches in machine learning and data analysis. Existing visual feature extractors, including ConvNets, ViTs, and MLPs, represent an image as rectangular regions. Though prevalent, such a grid-style paradigm is built upon engineering practice and lacks explicit modeling of data distribution. In this work, we propose feature extraction with clustering (FEC), a conceptually elegant yet surprisingly ad-hoc interpretable neural clustering framework, which views feature extraction as a process of selecting representatives from data and thus automatically captures the underlying data distribution. Given an image, FEC alternates between grouping pixels into individual clusters to abstract representatives and updating the deep features of pixels with current representatives. Such an iterative working mechanism is implemented in the form of several neural layers and the final representatives can be used for downstream tasks. The cluster assignments across layers, which can be viewed and inspected by humans, make the forward process of FEC fully transparent and empower it with promising ad-hoc interpretability. Extensive experiments on various visual recognition models and tasks verify the effectiveness, generality, and interpretability of FEC. We expect this work will provoke a rethink of the current de facto grid-style paradigm.","Tue, 26 Mar 2024 06:04:50 UTC (1,157 KB)"
"93","Generalization Error Analysis for Sparse Mixture-of-Experts: A Preliminary Study","Jinze Zhao, Peihao Wang, Zhangyang Wang","Machine Learning (cs.LG)","Mixture-of-Experts (MoE) represents an ensemble methodology that amalgamates predictions from several specialized sub-models (referred to as experts). This fusion is accomplished through a router mechanism, dynamically assigning weights to each expert's contribution based on the input data. Conventional MoE mechanisms select all available experts, incurring substantial computational costs. In contrast, Sparse Mixture-of-Experts (Sparse MoE) selectively engages only a limited number, or even just one expert, significantly reducing computation overhead while empirically preserving, and sometimes even enhancing, performance. Despite its wide-ranging applications and these advantageous characteristics, MoE's theoretical underpinnings have remained elusive. In this paper, we embark on an exploration of Sparse MoE's generalization error concerning various critical factors. Specifically, we investigate the impact of the number of data samples, the total number of experts, the sparsity in expert selection, the complexity of the routing mechanism, and the complexity of individual experts. Our analysis sheds light on \textit{how \textbf{sparsity} contributes to the MoE's generalization}, offering insights from the perspective of classical learning theory.","Tue, 26 Mar 2024 05:48:02 UTC (85 KB)"
"94","Handling multivariable missing data in causal mediation analysis","S. Ghazaleh Dashti, Katherine J. Lee, Julie A. Simpson, John B. Carlin, Margarita Moreno-Betancur","Applications (stat.AP)","Mediation analysis is commonly used in epidemiological research, but guidance is lacking on how multivariable missing data should be dealt with in these analyses. Multiple imputation (MI) is a widely used approach, but questions remain regarding impact of missingness mechanism, how to ensure imputation model compatibility and approaches to variance estimation. To address these gaps, we conducted a simulation study based on the Victorian Adolescent Health Cohort Study. We considered six missingness mechanisms, involving varying assumptions regarding the influence of outcome and/or mediator on missingness in key variables. We compared the performance of complete-case analysis, seven MI approaches, differing in how the imputation model was tailored, and a ""substantive model compatible"" MI approach. We evaluated both the MI-Boot (MI, then bootstrap) and Boot-MI (bootstrap, then MI) approaches to variance estimation. Results showed that when the mediator and/or outcome influenced their own missingness, there was large bias in effect estimates, while for other mechanisms appropriate MI approaches yielded approximately unbiased estimates. Beyond incorporating all analysis variables in the imputation model, how MI was tailored for compatibility with mediation analysis did not greatly impact point estimation bias. BootMI returned variance estimates with smaller bias than MIBoot, especially in the presence of incompatibility.","Tue, 26 Mar 2024 05:25:29 UTC (2,062 KB)"
"95","Explainable Graph Neural Networks for Observation Impact Analysis in Atmospheric State Estimation","Hyeon-Ju Jeon, Jeon-Ho Kang, In-Hyuk Kwon, O-Joun Lee","Artificial Intelligence (cs.AI)","This paper investigates the impact of observations on atmospheric state estimation in weather forecasting systems using graph neural networks (GNNs) and explainability methods. We integrate observation and Numerical Weather Prediction (NWP) points into a meteorological graph, extracting $k$-hop subgraphs centered on NWP points. Self-supervised GNNs are employed to estimate the atmospheric state by aggregating data within these $k$-hop radii. The study applies gradient-based explainability methods to quantify the significance of different observations in the estimation process. Evaluated with data from 11 satellite and land-based observations, the results highlight the effectiveness of visualizing the importance of observation types, enhancing the understanding and optimization of observational data in weather forecasting.","Tue, 26 Mar 2024 05:10:47 UTC (1,554 KB)"
"96","Exploring and Applying Audio-Based Sentiment Analysis in Music","Etash Jhanji","Sound (cs.SD)","Sentiment analysis is a continuously explored area of text processing that deals with the computational analysis of opinions, sentiments, and subjectivity of text. However, this idea is not limited to text and speech, in fact, it could be applied to other modalities. In reality, humans do not express themselves in text as deeply as they do in music. The ability of a computational model to interpret musical emotions is largely unexplored and could have implications and uses in therapy and musical queuing. In this paper, two individual tasks are addressed. This study seeks to (1) predict the emotion of a musical clip over time and (2) determine the next emotion value after the music in a time series to ensure seamless transitions. Utilizing data from the Emotions in Music Database, which contains clips of songs selected from the Free Music Archive annotated with levels of valence and arousal as reported on Russel's circumplex model of affect by multiple volunteers, models are trained for both tasks. Overall, the performance of these models reflected that they were able to perform the tasks they were designed for effectively and accurately.","Thu, 22 Feb 2024 22:34:06 UTC (4,723 KB)"
"97","On the Heating of the Slow Solar-Wind by Imbalanced Alfvén-Wave Turbulence from 0.06 au to 1 au: Parker Solar Probe and Solar Orbiter observations","Sofiane Bourouaine, Jean C. Perez, Benjamin D. G. Chandran, Vamsee K. Jagarlamudi, Nour E. Raouafi, Jasper S. Halekas","Space Physics (physics.space-ph)","In this work we analyze plasma and magnetic field data provided by the Parker Solar Probe (\emph{PSP}) and Solar Orbiter (\emph{SO}) missions to investigate the radial evolution of the heating of Alfvénic slow wind (ASW) by imbalanced Alfvén-Wave (AW) turbulent fluctuations from 0.06 au to 1 au. in our analysis we focus on slow solar-wind intervals with highly imbalanced and incompressible turbulence (i.e., magnetic compressibility $C_B=\delta B/B\leq 0.25$, plasma compressibility $C_n=\delta n/n\leq 0.25$ and normalized cross-helicity $\sigma_c\geq 0.65$). First, we estimate the AW turbulent dissipation rate from the wave energy equation and find that the radial profile trend is similar to the proton heating rate. Second, we find that the scaling of the empirical AW turbulent dissipation rate $Q_W$ obtained from the wave energy equation matches the scaling from the phenomenological AW turbulent dissipation rate $Q_{\rm CH09}$ (with $Q_{\rm CH09}\simeq 1.55 Q_W$) derived by~\cite{chandran09} based on the model of reflection-driven turbulence. Our results suggest that, as in the fast solar wind, AW turbulence plays a major role in the ion heating that occurs in incompressible slow-wind streams.","Tue, 26 Mar 2024 03:31:24 UTC (543 KB)"
"98","Measurement Uncertainty Impact on Koopman Operator Estimation of Power System Dynamics","P. Algikar, P. Sharma, M. Netto, L. Mili","Applications (stat.AP)","Sensor measurements are mission-critical for monitoring and controlling power systems because they provide real-time insight into the grid operating condition; however, confidence in these insights depends greatly on the quality of the sensor data. Uncertainty in sensor measurements is an intrinsic aspect of the measurement process. In this paper, we develop an analytical method to quantify the impact of measurement uncertainties in numerical methods that employ the Koopman operator to identify nonlinear dynamics based on recorded data. In particular, we quantify the confidence interval of each element in the push-forward matrix from which a subset of the Koopman operator's discrete spectrum is estimated. We provide a detailed numerical analysis of the developed method applied to numerical simulations and field data collected from experiments conducted in a megawatt-scale facility at the National Renewable Energy Laboratory.","Tue, 26 Mar 2024 02:53:26 UTC (4,822 KB)"
"99","Labeling subtypes in a Parkinson's Cohort using Multifeatures in MRI - Integrating Grey and White Matter Information","Tanmayee Samantaray, Jitender Saini, Pramod Kumar Pal, Bithiah Grace Jaganathan, Vijaya V Saradhi, Gupta CN","Image and Video Processing (eess.IV)","Thresholding of networks has long posed a challenge in brain connectivity analysis. Weighted networks are typically binarized using threshold measures to facilitate network analysis. Previous studies on MRI-based brain networks have predominantly utilized density or sparsity-based thresholding techniques, optimized within specific ranges derived from network metrics such as path length, clustering coefficient, and small-world index. Thus, determination of a single threshold value for facilitating comparative analysis of networks remains elusive. To address this, our study introduces Mutual K-Nearest Neighbor (MKNN)-based thresholding for brain network analysis. Here, nearest neighbor selection is based on the highest correlation between features of brain regions. Construction of brain networks was accomplished by computing Pearson correlations between grey matter volume and white matter volume for each pair of brain regions. Structural MRI data from 180 Parkinsons patients and 70 controls from the NIMHANS, India were analyzed. Subtypes within Parkinsons disease were identified based on grey and white matter volume atrophy using source-based morphometric decomposition. The loading coefficients were correlated with clinical features to discern clinical relationship with the deciphered subtypes. Our data-mining approach revealed: Subtype A (N = 51, intermediate type), Subtype B (N = 57, mild-severe type with mild motor symptoms), and Subtype AB (N = 36, most-severe type with predominance in motor impairment). Subtype-specific weighted matrices were binarized using MKNN-based thresholding for brain network analysis. Permutation tests on network metrics of resulting bipartite graphs demonstrated significant group differences in betweenness centrality and participation coefficient. The identified hubs were specific to each subtype, with some hubs conserved across different subtypes.","Tue, 26 Mar 2024 02:32:52 UTC (2,741 KB)"
"100","FedMIL: Federated-Multiple Instance Learning for Video Analysis with Optimized DPP Scheduling","Ashish Bastola, Hao Wang, Xiwen Chen, Abolfazl Razi","Distributed, Parallel, and Cluster Computing (cs.DC)","Many AI platforms, including traffic monitoring systems, use Federated Learning (FL) for decentralized sensor data processing for learning-based applications while preserving privacy and ensuring secured information transfer. On the other hand, applying supervised learning to large data samples, like high-resolution images requires intensive human labor to label different parts of a data sample. Multiple Instance Learning (MIL) alleviates this challenge by operating over labels assigned to the 'bag' of instances. In this paper, we introduce Federated Multiple-Instance Learning (FedMIL). This framework applies federated learning to boost the training performance in video-based MIL tasks such as vehicle accident detection using distributed CCTV networks. However, data sources in decentralized settings are not typically Independently and Identically Distributed (IID), making client selection imperative to collectively represent the entire dataset with minimal clients. To address this challenge, we propose DPPQ, a framework based on the Determinantal Point Process (DPP) with a quality-based kernel to select clients with the most diverse datasets that achieve better performance compared to both random selection and current DPP-based client selection methods even with less data utilization in the majority of non-IID cases. This offers a significant advantage for deployment on edge devices with limited computational resources, providing a reliable solution for training AI models in massive smart sensor networks.","Tue, 26 Mar 2024 02:30:50 UTC (4,187 KB)"
"101","Testing the $\mathbfΛ$CDM Cosmological Model with Forthcoming Measurements of the Cosmic Microwave Background with SPT-3G","K. Prabhu, S. Raghunathan, M. Millea, G. Lynch, P. A. R. Ade, E. Anderes, A. J. Anderson, B. Ansarinejad, M. Archipley, L. Balkenhol, K. Benabed, A. N. Bender, B. A. Benson, F. Bianchini, L. E. Bleem, F. R. Bouchet, L. Bryant, E. Camphuis, J. E. Carlstrom, T. W. Cecil, C. L. Chang, P. Chaubal, P. M. Chichura, T.-L. Chou, A. Coerver, T. M. Crawford, A. Cukierman, C. Daley, T. de Haan, K. R. Dibert, M. A. Dobbs, A. Doussot, D. Dutcher, W. Everett, C. Feng, K. R. Ferguson, K. Fichman, A. Foster, S. Galli, A. E. Gambrel, R. W. Gardner, F. Ge, N. Goeckner-Wald, R. Gualtieri, F. Guidi, S. Guns, N. W. Halverson, E. Hivon, G. P. Holder, W. L. Holzapfel, J. C. Hood, A. Hryciuk, N. Huang, F. Kéruzoré, L. Knox, M. Korman, K. Kornoelje, C.-L. Kuo, A. T. Lee, K. Levy, A. E. Lowitz, C. Lu, A. Maniyar, F. Menanteau, J. Montgomery, Y. Nakato, T. Natoli, G. I. Noble, V. Novosad, Y. Omori, S. Padin, Z. Pan, P. Paschos, K. A. Phadke, W. Quan, M. Rahimi, A. Rahlin, C. L. Reichardt, M. Rouble, J. E. Ruhl, E. Schiappucci, G. Smecher, J. A. Sobrin, A. A. Stark, J. Stephen, A. Suzuki, C. Tandoi, K. L. Thompson, B. Thorne, C. Trendafilova, C. Tucker, C. Umilta, A. Vitrier, J. D. Vieira, Y. Wan, G. Wang, N. Whitehorn, W. L. K. Wu, V. Yefremenko, M. R. Young, J. A. Zebrowski","Cosmology and Nongalactic Astrophysics (astro-ph.CO)","We forecast constraints on cosmological parameters enabled by three surveys conducted with SPT-3G, the third-generation camera on the South Pole Telescope. The surveys cover separate regions of 1500, 2650, and 6000 ${\rm deg}^{2}$ to different depths, in total observing 25% of the sky. These regions will be measured to white noise levels of roughly 2.5, 9, and 12 $\mu{\rm K-arcmin}$, respectively, in CMB temperature units at 150 GHz by the end of 2024. The survey also includes measurements at 95 and 220 GHz, which have noise levels a factor of ~1.2 and 3.5 times higher than 150 GHz, respectively, with each band having a polarization noise level ~$\sqrt{\text{2}}$ times higher than the temperature noise. We use a novel approach to obtain the covariance matrices for jointly and optimally estimated gravitational lensing potential bandpowers and unlensed CMB temperature and polarization bandpowers. We demonstrate the ability to test the $\Lambda{\rm CDM}$ model via the consistency of cosmological parameters constrained independently from SPT-3G and Planck data, and consider the improvement in constraints on $\Lambda{\rm CDM}$ extension parameters from a joint analysis of SPT-3G and Planck data. The $\Lambda{\rm CDM}$ cosmological parameters are typically constrained with uncertainties up to ~2 times smaller with SPT-3G data, compared to Planck, with the two data sets measuring significantly different angular scales and polarization levels, providing additional tests of the standard cosmological model.","Tue, 26 Mar 2024 17:57:20 UTC (4,338 KB)"
"102","Empowering Data Mesh with Federated Learning","Haoyuan Li, Salman Toor","Machine Learning (cs.LG)","The evolution of data architecture has seen the rise of data lakes, aiming to solve the bottlenecks of data management and promote intelligent decision-making. However, this centralized architecture is limited by the proliferation of data sources and the growing demand for timely analysis and processing. A new data paradigm, Data Mesh, is proposed to overcome these challenges. Data Mesh treats domains as a first-class concern by distributing the data ownership from the central team to each data domain, while keeping the federated governance to monitor domains and their data products. Many multi-million dollar organizations like Paypal, Netflix, and Zalando have already transformed their data analysis pipelines based on this new architecture. In this decentralized architecture where data is locally preserved by each domain team, traditional centralized machine learning is incapable of conducting effective analysis across multiple domains, especially for security-sensitive organizations. To this end, we introduce a pioneering approach that incorporates Federated Learning into Data Mesh. To the best of our knowledge, this is the first open-source applied work that represents a critical advancement toward the integration of federated learning methods into the Data Mesh paradigm, underscoring the promising prospects for privacy-preserving and decentralized data analysis strategies within Data Mesh architecture.","Tue, 26 Mar 2024 17:10:15 UTC (929 KB)"
"103","To Supervise or Not to Supervise: Understanding and Addressing the Key Challenges of 3D Transfer Learning","Souhail Hadgi, Lei Li, Maks Ovsjanikov","Computer Vision and Pattern Recognition (cs.CV)","Transfer learning has long been a key factor in the advancement of many fields including 2D image analysis. Unfortunately, its applicability in 3D data processing has been relatively limited. While several approaches for 3D transfer learning have been proposed in recent literature, with contrastive learning gaining particular prominence, most existing methods in this domain have only been studied and evaluated in limited scenarios. Most importantly, there is currently a lack of principled understanding of both when and why 3D transfer learning methods are applicable. Remarkably, even the applicability of standard supervised pre-training is poorly understood. In this work, we conduct the first in-depth quantitative and qualitative investigation of supervised and contrastive pre-training strategies and their utility in downstream 3D tasks. We demonstrate that layer-wise analysis of learned features provides significant insight into the downstream utility of trained networks. Informed by this analysis, we propose a simple geometric regularization strategy, which improves the transferability of supervised pre-training. Our work thus sheds light onto both the specific challenges of 3D transfer learning, as well as strategies to overcome them.","Tue, 26 Mar 2024 16:57:33 UTC (5,797 KB)"
"104","Using Domain Knowledge to Guide Dialog Structure Induction via Neural Probabilistic Soft Logic","Connor Pryor, Quan Yuan, Jeremiah Liu, Mehran Kazemi, Deepak Ramachandran, Tania Bedrax-Weiss, Lise Getoor","Computation and Language (cs.CL)","Dialog Structure Induction (DSI) is the task of inferring the latent dialog structure (i.e., a set of dialog states and their temporal transitions) of a given goal-oriented dialog. It is a critical component for modern dialog system design and discourse analysis. Existing DSI approaches are often purely data-driven, deploy models that infer latent states without access to domain knowledge, underperform when the training corpus is limited/noisy, or have difficulty when test dialogs exhibit distributional shifts from the training domain. This work explores a neural-symbolic approach as a potential solution to these problems. We introduce Neural Probabilistic Soft Logic Dialogue Structure Induction (NEUPSL DSI), a principled approach that injects symbolic knowledge into the latent space of a generative neural model. We conduct a thorough empirical investigation on the effect of NEUPSL DSI learning on hidden representation quality, few-shot learning, and out-of-domain generalization performance. Over three dialog structure induction datasets and across unsupervised and semi-supervised settings for standard and cross-domain generalization, the injection of symbolic knowledge using NEUPSL DSI provides a consistent boost in performance over the canonical baselines.","Tue, 26 Mar 2024 16:42:30 UTC (3,380 KB)"
"105","Learning the Optimal Power Flow: Environment Design Matters","Thomas Wolgast, Astrid Nieße","Machine Learning (cs.LG)","To solve the optimal power flow (OPF) problem, reinforcement learning (RL) emerges as a promising new approach. However, the RL-OPF literature is strongly divided regarding the exact formulation of the OPF problem as an RL environment. In this work, we collect and implement diverse environment design decisions from the literature regarding training data, observation space, episode definition, and reward function choice. In an experimental analysis, we show the significant impact of these environment design options on RL-OPF training performance. Further, we derive some first recommendations regarding the choice of these design decisions. The created environment framework is fully open-source and can serve as a benchmark for future research in the RL-OPF field.","Tue, 26 Mar 2024 16:13:55 UTC (177 KB)"
"106","Band Structure and Fermi Surface Nesting in $LaSb_2$","Evan O'Leary, Lin-Lin Wang, Yevhen Kushnirenko, Ben Schrunk, Andrew Eaton, Paula Herrera-Siklody, Paul C. Canfield, Adam Kaminski","Strongly Correlated Electrons (cond-mat.str-el)","We use high-resolution angle resolved photoemission spectroscopy (ARPES) and density functional theory (DFT) to investigate the electronic structure of the charge density wave (CDW) system LaSb$_2$. This compound is among an interesting group of materials that manifests both a CDW transition and lower temperature superconductivity. We find the DFT calculations to be in good agreement with our ARPES data. The Fermi surface of LaSb$_2$ consists of two small hole pockets close to $\Gamma$ and four larger pockets near the Brillouin zone (BZ) boundary. The overall features of the Fermi surface do not vary with temperature. A saddle point is present at -0.19 $eV$ below the Fermi level at $\Gamma$. Critical points in band structure have more pronounced effects on a materials properties when they are located closer to the Fermi level, making doped LaSb$_2$ compounds a potential interesting subject of future research. Multiple peaks are present in the generalized, electronic susceptibility calculations indicating the presence of possible nesting vectors. We were not able to detect any signatures of the CDW transition at 355 K, pointing to the subtle nature of this transition. This is unusual, given that such a high transition temperature is expected to be associated with the presence of a large CDW gap. This is confirmed through investigation of the Fermi surface and through analysis of momentum distribution curves (MDC). It is possible that changes are subtle and occur below current sensitivity of our measurements.","Tue, 26 Mar 2024 16:06:07 UTC (9,603 KB)"
"107","Towards Multilevel Modelling of Train Passing Events on the Staffordshire Bridge","Lawrence A. Bull, Chiho Jeon, Mark Girolami, Andrew Duncan, Jennifer Schooling, Miguel Bravo Haro","Applications (stat.AP)","We suggest a multilevel model, to represent aggregate train-passing events from the Staffordshire bridge monitoring system. We formulate a combined model from simple units, representing strain envelopes (of each train passing) for two types of commuter train. The measurements are treated as a longitudinal dataset and represented with a (low-rank approximation) hierarchical Gaussian process. For each unit in the combined model, we encode domain expertise as boundary condition constraints and work towards a general representation of the strain response. Looking forward, this should allow for the simulation of train types that were previously unobserved in the training data. For example, trains with more passengers or freights with a heavier payload. The strain event simulations are valuable since they can inform further experiments (including FEM calibration, fatigue analysis, or design) to test the bridge in hypothesised scenarios.","Tue, 26 Mar 2024 15:55:54 UTC (4,823 KB)"
"108","CSSTs: A Dynamic Data Structure for Partial Orders in Concurrent Execution Analysis","Hünkar Can Tunç, Ameya Prashant Deshmukh, Berk Çirisci, Constantin Enea, Andreas Pavlogiannis","Programming Languages (cs.PL)","Dynamic analyses are a standard approach to analyzing and testing concurrent programs. Such techniques observe program traces and analyze them to infer the presence or absence of bugs. At its core, each analysis maintains a partial order $P$ that represents order dependencies between events of the analyzed trace $\sigma$. Naturally, the scalability of the analysis largely depends on how efficiently it maintains $P$. The standard data structure for this task has thus far been vector clocks. These, however, are slow for analyses that follow a non-streaming style, costing $O(n)$ for inserting (and propagating) each new ordering in $P$, where $n$ is the size of $\sigma$, while they cannot handle the deletion of existing orderings.
In this paper we develop collective sparse segment trees (CSSTs), a simple but elegant data structure for generically maintaining a partial order $P$. CSSTs thrive when the width $k$ of $P$ is much smaller than the size $n$ of its domain, allowing inserting, deleting, and querying for orderings in $P$ to run in $O(logn)$ time. For a concurrent trace, $k$ is bounded by the number of its threads, and is normally orders of magnitude smaller than its size $n$, making CSSTs fitting for this setting. Our experimental results confirm that CSSTs are the best data structure currently to handle a range of dynamic analyses from existing literature.","Tue, 26 Mar 2024 15:53:53 UTC (124 KB)"
"109","Probing the shape of the quark-gluon plasma droplet via event-by-event QGP tomography","Bithika Karmakar, Dusan Zigic, Pasi Huovinen, Marko Djordjevic, Magdalena Djordjevic, Jussi Auvinen","High Energy Physics - Phenomenology (hep-ph)","This study investigates Quark-Gluon Plasma (QGP) in heavy-ion collisions through two avenues: high-$p_{\perp}$ frameworks and hydrodynamic modeling. Using the T$_{\text{R}}$ENTo model, we find that IP-Glasma mimicking $p=0$ value aligns well with high-$p_{\perp}$ data, in agreement with Bayesian analysis of the low-$p_{\perp}$ regime. While adjusting $p$ values may improve a fit to a particular high-$p_{\perp}$ observable, it does not permit an earlier onset of transverse expansion.","Tue, 26 Mar 2024 15:53:07 UTC (884 KB)"
"110","Graph Language Model (GLM): A new graph-based approach to detect social instabilities","Wallyson Lemes de Oliveira, Vahid Shamsaddini, Ali Ghofrani, Rahul Singh Inda, Jithendra Sai Veeramaneni, Étienne Voutaz","Computation and Language (cs.CL)","This scientific report presents a novel methodology for the early prediction of important political events using News datasets. The methodology leverages natural language processing, graph theory, clique analysis, and semantic relationships to uncover hidden predictive signals within the data. Initially, we designed a preliminary version of the method and tested it on a few events. This analysis revealed limitations in the initial research phase. We then enhanced the model in two key ways: first, we added a filtration step to only consider politically relevant news before further processing; second, we adjusted the input features to make the alert system more sensitive to significant spikes in the data. After finalizing the improved methodology, we tested it on eleven events including US protests, the Ukraine war, and French protests. Results demonstrate the superiority of our approach compared to baseline methods. Through targeted refinements, our model can now provide earlier and more accurate predictions of major political events based on subtle patterns in news data.","Tue, 26 Mar 2024 15:53:02 UTC (15,022 KB)"
"111","Exploring the Boundaries of Ambient Awareness in Twitter","Pablo Sanchez-Martin, Sonja Utz, Isabel Valera","Social and Information Networks (cs.SI)","Ambient awareness refers to the ability of social media users to obtain knowledge about who knows what (i.e., users' expertise) in their network, by simply being exposed to other users' content (e.g, tweets on Twitter). Previous work, based on user surveys, reveals that individuals self-report ambient awareness only for parts of their networks. However, it is unclear whether it is their limited cognitive capacity or the limited exposure to diagnostic tweets (i.e., online content) that prevents people from developing ambient awareness for their complete network. In this work, we focus on in-wall ambient awareness (IWAA) in Twitter and conduct a two-step data-driven analysis, that allows us to explore to which extent IWAA is likely, or even possible. First, we rely on reactions (e.g., likes), as strong evidence of users being aware of experts in Twitter. Unfortunately, such strong evidence can be only measured for active users, which represent the minority in the network. Thus to study the boundaries of IWAA to a larger extent, in the second part of our analysis, we instead focus on the passive exposure to content generated by other users -- which we refer to as in-wall visibility. This analysis shows that (in line with \citet{levordashka2016ambient}) only for a subset of users IWAA is plausible, while for the majority it is unlikely, if even possible, to develop IWAA. We hope that our methodology paves the way for the emergence of data-driven approaches for the study of ambient awareness.","Tue, 26 Mar 2024 15:09:33 UTC (2,405 KB)"
"112","Noise2Noise Denoising of CRISM Hyperspectral Data","Robert Platt, Rossella Arcucci, Cédric M. John","Computer Vision and Pattern Recognition (cs.CV)","Hyperspectral data acquired by the Compact Reconnaissance Imaging Spectrometer for Mars (CRISM) have allowed for unparalleled mapping of the surface mineralogy of Mars. Due to sensor degradation over time, a significant portion of the recently acquired data is considered unusable. Here a new data-driven model architecture, Noise2Noise4Mars (N2N4M), is introduced to remove noise from CRISM images. Our model is self-supervised and does not require zero-noise target data, making it well suited for use in Planetary Science applications where high quality labelled data is scarce. We demonstrate its strong performance on synthetic-noise data and CRISM images, and its impact on downstream classification performance, outperforming benchmark methods on most metrics. This allows for detailed analysis for critical sites of interest on the Martian surface, including proposed lander sites.","Tue, 26 Mar 2024 14:49:22 UTC (3,683 KB)"
"113","Regularity for nonlocal equations with local Neumann boundary conditions","Xavier Ros-Oton, Marvin Weidner","Analysis of PDEs (math.AP)","In this article we establish fine results on the boundary behavior of solutions to nonlocal equations in $C^{k,\gamma}$ domains which satisfy local Neumann conditions on the boundary. Such solutions typically blow up at the boundary like $v \asymp d^{s-1}$ and are sometimes called large solutions. In this setup we prove optimal regularity results for the quotients $v/d^{s-1}$, depending on the regularity of the domain and on the data of the problem. The results of this article will be important in a forthcoming work on nonlocal free boundary problems.","Tue, 26 Mar 2024 14:11:19 UTC (46 KB)"
"114","Nuclear matrix elements of neutrinoless double-beta decay in covariant density functional theory with different mechanisms","C. R. Ding, Gang Li, J. M. Yao","Nuclear Theory (nucl-th)","Nuclear matrix elements (NMEs) for neutrinoless double-beta ($0\nu\beta\beta$) decay in candidate nuclei play a crucial role in interpreting results from current experiments and in designing future ones. Accurate NME values serve as important nuclear inputs for constraining parameters in new physics, such as neutrino mass and the Wilson coefficients of lepton-number-violating (LNV) operators. In this study, we present a comprehensive calculation of NMEs for $0\nu\beta\beta$ decay in $^{76}$Ge, $^{82}$Se, $^{100}$Mo, $^{130}$Te, and $^{136}$Xe, using nuclear wave functions obtained from multi-reference covariant density functional theory (MR-CDFT). We employ three types of transition potentials at the leading order in chiral effective field theory. Our results, along with recent data, are utilized to constrain the coefficients of LNV operators. We find that NMEs for the standard and short-range mechanisms are significantly larger than those for the non-standard long-range mechanism. The use of NMEs from various nuclear models does not notably change the parameter space intervals for the coefficients, although MR-CDFT yields the most stringent constraint. Furthermore, our NMEs can also be used to perform a more comprehensive analysis with multiple isotopes.","Tue, 26 Mar 2024 14:06:23 UTC (2,985 KB)"
"115","Low-temperature benchmarking of qubit control wires by primary electron thermometry","Elias Roos Hansen, Ferdinand Kuemmeth, Joost van der Heijden","Mesoscale and Nanoscale Physics (cond-mat.mes-hall)","Low-frequency qubit control wires require non-trivial thermal anchoring and low-pass filtering. The resulting electron temperature serves as a quality benchmark for these signal lines. In this technical note, we make use of a primary electron thermometry technique, using a Coulomb blockade thermometer, to establish the electron temperature in the millikelvin regime. The experimental four-probe measurement setup, the data analysis, and the measurement limitations are discussed in detail. We verify the results by also using another electron thermometry technique, based on a superconductor-insulator-normal metal junction. Our comparison of signal lines with QDevil's QFilter to unfiltered signal lines demonstrates that the filter significantly reduces both the rms noise and electron temperature, which is measured to be 22 $\pm$ 1 mK.","Tue, 26 Mar 2024 14:03:19 UTC (8,966 KB)"
"116","Panonut360: A Head and Eye Tracking Dataset for Panoramic Video","Yutong Xu, Junhao Du, Jiahe Wang, Yuwei Ning, Sihan Zhou Yang Cao","Computer Vision and Pattern Recognition (cs.CV)","With the rapid development and widespread application of VR/AR technology, maximizing the quality of immersive panoramic video services that match users' personal preferences and habits has become a long-standing challenge. Understanding the saliency region where users focus, based on data collected with HMDs, can promote multimedia encoding, transmission, and quality assessment. At the same time, large-scale datasets are essential for researchers and developers to explore short/long-term user behavior patterns and train AI models related to panoramic videos. However, existing panoramic video datasets often include low-frequency user head or eye movement data through short-term videos only, lacking sufficient data for analyzing users' Field of View (FoV) and generating video saliency regions.
Driven by these practical factors, in this paper, we present a head and eye tracking dataset involving 50 users (25 males and 25 females) watching 15 panoramic videos. The dataset provides details on the viewport and gaze attention locations of users. Besides, we present some statistics samples extracted from the dataset. For example, the deviation between head and eye movements challenges the widely held assumption that gaze attention decreases from the center of the FoV following a Gaussian distribution. Our analysis reveals a consistent downward offset in gaze fixations relative to the FoV in experimental settings involving multiple users and videos. That's why we name the dataset Panonut, a saliency weighting shaped like a donut. Finally, we also provide a script that generates saliency distributions based on given head or eye coordinates and pre-generated saliency distribution map sets of each video from the collected eye tracking data.
The dataset is available on website: this https URL.","Tue, 26 Mar 2024 13:54:52 UTC (11,373 KB)"
"117","Not All Similarities Are Created Equal: Leveraging Data-Driven Biases to Inform GenAI Copyright Disputes","Uri Hacohen, Adi Haviv, Shahar Sarfaty, Bruria Friedman, Niva Elkin-Koren, Roi Livni, Amit H Bermano","Computer Vision and Pattern Recognition (cs.CV)","The advent of Generative Artificial Intelligence (GenAI) models, including GitHub Copilot, OpenAI GPT, and Stable Diffusion, has revolutionized content creation, enabling non-professionals to produce high-quality content across various domains. This transformative technology has led to a surge of synthetic content and sparked legal disputes over copyright infringement. To address these challenges, this paper introduces a novel approach that leverages the learning capacity of GenAI models for copyright legal analysis, demonstrated with GPT2 and Stable Diffusion models. Copyright law distinguishes between original expressions and generic ones (Scènes à faire), protecting the former and permitting reproduction of the latter. However, this distinction has historically been challenging to make consistently, leading to over-protection of copyrighted works. GenAI offers an unprecedented opportunity to enhance this legal analysis by revealing shared patterns in preexisting works. We propose a data-driven approach to identify the genericity of works created by GenAI, employing ""data-driven bias"" to assess the genericity of expressive compositions. This approach aids in copyright scope determination by utilizing the capabilities of GenAI to identify and prioritize expressive elements and rank them according to their frequency in the model's dataset. The potential implications of measuring expressive genericity for copyright law are profound. Such scoring could assist courts in determining copyright scope during litigation, inform the registration practices of Copyright Offices, allowing registration of only highly original synthetic works, and help copyright owners signal the value of their works and facilitate fairer licensing deals. More generally, this approach offers valuable insights to policymakers grappling with adapting copyright law to the challenges posed by the era of GenAI.","Tue, 26 Mar 2024 13:32:32 UTC (10,819 KB)"
"118","Assessing the similarity of real matrices with arbitrary shape","Jasper Albers, Anno C. Kurth, Robin Gutzen, Aitor Morales-Gregorio, Michael Denker, Sonja Grün, Sacha J. van Albada, Markus Diesmann","Neurons and Cognition (q-bio.NC)","Assessing the similarity of matrices is valuable for analyzing the extent to which data sets exhibit common features in tasks such as data clustering, dimensionality reduction, pattern recognition, group comparison, and graph analysis. Methods proposed for comparing vectors, such as cosine similarity, can be readily generalized to matrices. However, this approach usually neglects the inherent two-dimensional structure of matrices. Here, we propose singular angle similarity (SAS), a measure for evaluating the structural similarity between two arbitrary, real matrices of the same shape based on singular value decomposition. After introducing the measure, we compare SAS with standard measures for matrix comparison and show that only SAS captures the two-dimensional structure of matrices. Further, we characterize the behavior of SAS in the presence of noise and as a function of matrix dimensionality. Finally, we apply SAS to two use cases: square non-symmetric matrices of probabilistic network connectivity, and non-square matrices representing neural brain activity. For synthetic data of network connectivity, SAS matches intuitive expectations and allows for a robust assessment of similarities and differences. For experimental data of brain activity, SAS captures differences in the structure of high-dimensional responses to different stimuli. We conclude that SAS is a suitable measure for quantifying the shared structure of matrices with arbitrary shape.","Tue, 26 Mar 2024 13:24:52 UTC (1,141 KB)"
"119","Analysis on reservoir activation with the nonlinearity harnessed from solution-processed MoS2 devices","Songwei Liu, Yang Liu, Yingyi Wen, Jingfang Pei, Pengyu Liu, Lekai Song, Xiaoyue Fan, Wenchen Yang, Danmei Pan, Teng Ma, Yue Lin, Gang Wang, Guohua Hu","Applied Physics (physics.app-ph)","Reservoir computing is a recurrent neural network that has been applied across various domains in machine learning. The implementation of reservoir computing, however, often demands heavy computations for activating the reservoir. Configuring physical reservoir networks and harnessing the nonlinearity from the underlying devices for activation is an emergent solution to address the computational challenge. Herein, we analyze the feasibility of employing the nonlinearity from solution-processed molybdenum disulfide (MoS2) devices for reservoir activation. The devices, fabricated using liquid-phase exfoliated MoS2, exhibit a high-order nonlinearity achieved by Stark modulation of the MoS2 material. We demonstrate that this nonlinearity can be fitted and employed as the activation function to facilitate reservoir computing implementation. Notably, owing to the high-order nonlinearity, the network exhibits long-term synchronization and robust generalization abilities for approximating complex dynamical systems. Given the remarkable reservoir activation capability, coupled with the scalability of the device fabrication, our findings open the possibility for the physical realization of lightweight, efficient reservoir computing for, for instance, signal classification, motion tracking, and pattern recognition of complex time series as well as secure cryptography. As an example, we show the network can be appointed to generate chaotic random numbers for secure data encryption.","Tue, 26 Mar 2024 13:04:00 UTC (2,609 KB)"
"120","How Private is DP-SGD?","Lynn Chua, Badih Ghazi, Pritish Kamath, Ravi Kumar, Pasin Manurangsi, Amer Sinha, Chiyuan Zhang","Machine Learning (cs.LG)","We demonstrate a substantial gap between the privacy guarantees of the Adaptive Batch Linear Queries (ABLQ) mechanism under different types of batch sampling: (i) Shuffling, and (ii) Poisson subsampling; the typical analysis of Differentially Private Stochastic Gradient Descent (DP-SGD) follows by interpreting it as a post-processing of ABLQ. While shuffling based DP-SGD is more commonly used in practical implementations, it is neither analytically nor numerically amenable to easy privacy analysis. On the other hand, Poisson subsampling based DP-SGD is challenging to scalably implement, but has a well-understood privacy analysis, with multiple open-source numerically tight privacy accountants available. This has led to a common practice of using shuffling based DP-SGD in practice, but using the privacy analysis for the corresponding Poisson subsampling version. Our result shows that there can be a substantial gap between the privacy analysis when using the two types of batch sampling, and thus advises caution in reporting privacy parameters for DP-SGD.","Tue, 26 Mar 2024 13:02:43 UTC (684 KB)"
"121","Constraining Primordial Non-Gaussianity from Large Scale Structure with the Wavelet Scattering Transform","Matteo Peron, Gabriel Jung, Michele Liguori, Massimo Pietroni","Cosmology and Nongalactic Astrophysics (astro-ph.CO)","We investigate the Wavelet Scattering Transform (WST) as a tool for the study of Primordial non-Gaussianity (PNG) in Large Scale Structure (LSS), and compare its performance with that achievable via a joint analysis with power spectrum and bispectrum (P+B). We consider the three main primordial bispectrum shapes - local, equilateral and orthogonal - and produce Fisher forecast for the corresponding fNL amplitude parameters, jointly with standard cosmological parameters. We analyze simulations from the publicly available ""Quijote"" and ""Quijote-png"" N-body suites, studying both the dark matter and halo fields. We find that the WST outperforms the power spectrum alone on all parameters, both on the fNL's and on cosmological ones. In particular, on fNL_loc for halos, the improvement is about 27%. When B is combined with P, halo constraints from WST are weaker for fNL_loc (at ~ 15% level), but stronger for fNL_eq (~ 25%) and fNL_ortho (~ 28%). Our results show that WST, both alone and in combination with P+B, can improve the extraction of information on PNG from LSS data over the one attainable by a standard P+B analysis. Moreover, we identify a class of WST in which the origin of the extra information on PNG can be cleanly isolated.","Tue, 26 Mar 2024 12:40:49 UTC (18,902 KB)"
"122","Microscale Morphology Driven Thermal Transport in Fiber Reinforced Polymer Composites","Sabarinathan P Subramaniyan, Jonathan D Boehnlein, Pavana Prabhakar","Applied Physics (physics.app-ph)","Fiber-reinforced polymer composite (FRPC) materials are used extensively in various industries, such as aerospace, automobiles, and electronics packaging, due to their remarkable specific strength and desirable properties, such as enhanced durability and corrosion resistance. The evolution of thermal properties in FRPCs is crucial for advancing thermal management systems, optimizing material performance, and enhancing energy efficiency across these diverse sectors. Despite significant research efforts to develop new materials with improved thermal properties and reduced thermal degradation, there is a lack of understanding of the thermal transport phenomena considering the influence of microscale reinforcement morphology in these composites. In the current study, we performed experimental investigations complemented by computations to determine the thermal transport properties and associated phenomena in epoxy and carbon fiber-reinforced epoxy composites. The experimental findings were utilized as input data for numerical analysis to examine the impact of fiber morphology and volume fraction in thermal transport phenomena. Our results revealed that composites incorporating non-circular fibers manifested higher thermal conductivity than traditional circular fibers in the transverse direction. This can be attributed to increased interconnected heat flow pathways facilitated by the increased surface area of non-circular fibers with the same cross-sectional areas, resulting in efficient heat transfer.","Tue, 26 Mar 2024 12:31:33 UTC (1,801 KB)"
"123","Data-driven Energy Consumption Modelling for Electric Micromobility using an Open Dataset","Yue Ding, Sen Yan, Maqsood Hussain Shah, Hongyuan Fang, Ji Li, Mingming Liu","Artificial Intelligence (cs.AI)","The escalating challenges of traffic congestion and environmental degradation underscore the critical importance of embracing E-Mobility solutions in urban spaces. In particular, micro E-Mobility tools such as E-scooters and E-bikes, play a pivotal role in this transition, offering sustainable alternatives for urban commuters. However, the energy consumption patterns for these tools are a critical aspect that impacts their effectiveness in real-world scenarios and is essential for trip planning and boosting user confidence in using these. To this effect, recent studies have utilised physical models customised for specific mobility tools and conditions, but these models struggle with generalization and effectiveness in real-world scenarios due to a notable absence of open datasets for thorough model evaluation and verification. To fill this gap, our work presents an open dataset, collected in Dublin, Ireland, specifically designed for energy modelling research related to E-Scooters and E-Bikes. Furthermore, we provide a comprehensive analysis of energy consumption modelling based on the dataset using a set of representative machine learning algorithms and compare their performance against the contemporary mathematical models as a baseline. Our results demonstrate a notable advantage for data-driven models in comparison to the corresponding mathematical models for estimating energy consumption. Specifically, data-driven models outperform physical models in accuracy by up to 83.83% for E-Bikes and 82.16% for E-Scooters based on an in-depth analysis of the dataset under certain assumptions.","Tue, 26 Mar 2024 12:08:05 UTC (23,196 KB)"
"124","Online Tree Reconstruction and Forest Inventory on a Mobile Robotic System","Leonard Freißmuth, Matias Mattamala, Nived Chebrolu, Simon Schaefer, Stefan Leutenegger, Maurice Fallon","Robotics (cs.RO)","Terrestrial laser scanning (TLS) is the standard technique used to create accurate point clouds for digital forest inventories. However, the measurement process is demanding, requiring up to two days per hectare for data collection, significant data storage, as well as resource-heavy post-processing of 3D data. In this work, we present a real-time mapping and analysis system that enables online generation of forest inventories using mobile laser scanners that can be mounted e.g. on mobile robots. Given incrementally created and locally accurate submaps-data payloads-our approach extracts tree candidates using a custom, Voronoi-inspired clustering algorithm. Tree candidates are reconstructed using an adapted Hough algorithm, which enables robust modeling of the tree stem. Further, we explicitly incorporate the incremental nature of the data collection by consistently updating the database using a pose graph LiDAR SLAM system. This enables us to refine our estimates of the tree traits if an area is revisited later during a mission. We demonstrate competitive accuracy to TLS or manual measurements using laser scanners that we mounted on backpacks or mobile robots operating in conifer, broad-leaf and mixed forests. Our results achieve RMSE of 1.93 cm, a bias of 0.65 cm and a standard deviation of 1.81 cm (averaged across these sequences)-with no post-processing required after the mission is complete.","Tue, 26 Mar 2024 11:51:58 UTC (5,709 KB)"
"125","Lattice dynamics of LiNb$_{\text{1-x}}$Ta$_{\text{x}}$O$_{\text{3}}$ solid solutions: Theory and experiment","Felix Bernhardt, Soham Gharat, Alexander Kapp, Florian Pfeiffer, Robin Buschbeck, Franz Hempel, Oleksiy Pashkin, Susanne C. Kehr, Michael Rüsing, Simone Sanna, Lukas M. Eng","Materials Science (cond-mat.mtrl-sci)","Lithium niobate (LNO) and lithium tantalate (LTO) see widespread use in fundamental research and commercial technologies reaching from electronics over classical optics to integrated quantum communication. In recent years, the mixed crystal system lithium niobate tantalate (LNT) allows for the dedicate engineering of material properties by combining the advantages of the two parental materials LNO and LTO. Vibrational spectroscopies such as Raman spectroscopy or (Fourier transform) infrared spectroscopy are vital techniques to provide detailed insight into the material properties, which is central to the analysis and optimization of devices. In this work, we present a joint experimental-theoretical approach allowing to unambiguously assign the spectral features in the LNT material family through both Raman and IR spectroscopy, as well as to provide an in-depth explanation for the observed scattering efficiencies based on first-principles calculations. The phononic contribution to the static dielectric tensor is calculated from the experimental and theoretical data using the generalized Lyddane-Sachs-Teller relation and compared with the results of the first-principles calculations. The joint methodology can be readily expanded to other materials and serves, e.g., as the basis for studying the role of point defects or doping.","Tue, 26 Mar 2024 11:05:32 UTC (2,245 KB)"
"126","Parameterized Analysis of Bribery in Challenge the Champ Tournaments","Juhi Chaudhary, Hendrik Molter, Meirav Zehavi","Data Structures and Algorithms (cs.DS)","Challenge the champ tournaments are one of the simplest forms of competition, where a (initially selected) champ is repeatedly challenged by other players. If a player beats the champ, then that player is considered the new (current) champ. Each player in the competition challenges the current champ once in a fixed order. The champ of the last round is considered the winner of the tournament. We investigate a setting where players can be bribed to lower their winning probability against the initial champ. The goal is to maximize the probability of the initial champ winning the tournament by bribing the other players, while not exceeding a given budget for the bribes. Mattei et al. [Journal of Applied Logic, 2015] showed that the problem can be solved in pseudo-polynomial time, and that it is in XP when parameterized by the number of players.
We show that the problem is weakly NP-hard and W[1]-hard when parameterized by the number of players. On the algorithmic side, we show that the problem is fixed-parameter tractable when parameterized either by the number of different bribe values or the number of different probability values. To this end, we establish several results that are of independent interest. In particular, we show that the product knapsack problem is W[1]-hard when parameterized by the number of items in the knapsack, and that constructive bribery for cup tournaments is W[1]-hard when parameterized by the number of players. Furthermore, we present a novel way of designing mixed integer linear programs, ensuring optimal solutions where all variables are integers.","Tue, 26 Mar 2024 10:53:25 UTC (60 KB)"
"127","A Survey on Deep Learning and State-of-the-arts Applications","Mohd Halim Mohd Noor, Ayokunle Olalekan Ige","Machine Learning (cs.LG)","Deep learning, a branch of artificial intelligence, is a computational model that uses multiple layers of interconnected units (neurons) to learn intricate patterns and representations directly from raw input data. Empowered by this learning capability, it has become a powerful tool for solving complex problems and is the core driver of many groundbreaking technologies and innovations. Building a deep learning model is a challenging task due to the algorithm`s complexity and the dynamic nature of real-world problems. Several studies have reviewed deep learning concepts and applications. However, the studies mostly focused on the types of deep learning models and convolutional neural network architectures, offering limited coverage of the state-of-the-art of deep learning models and their applications in solving complex problems across different domains. Therefore, motivated by the limitations, this study aims to comprehensively review the state-of-the-art deep learning models in computer vision, natural language processing, time series analysis and pervasive computing. We highlight the key features of the models and their effectiveness in solving the problems within each domain. Furthermore, this study presents the fundamentals of deep learning, various deep learning model types and prominent convolutional neural network architectures. Finally, challenges and future directions in deep learning research are discussed to offer a broader perspective for future researchers.","Tue, 26 Mar 2024 10:10:53 UTC (1,990 KB)"
"128","Practical Applications of Advanced Cloud Services and Generative AI Systems in Medical Image Analysis","Jingyu Xu, Binbin Wu, Jiaxin Huang, Yulu Gong, Yifan Zhang, Bo Liu","Artificial Intelligence (cs.AI)","The medical field is one of the important fields in the application of artificial intelligence technology. With the explosive growth and diversification of medical data, as well as the continuous improvement of medical needs and challenges, artificial intelligence technology is playing an increasingly important role in the medical field. Artificial intelligence technologies represented by computer vision, natural language processing, and machine learning have been widely penetrated into diverse scenarios such as medical imaging, health management, medical information, and drug research and development, and have become an important driving force for improving the level and quality of medical services.The article explores the transformative potential of generative AI in medical imaging, emphasizing its ability to generate syntheticACM-2 data, enhance images, aid in anomaly detection, and facilitate image-to-image translation. Despite challenges like model complexity, the applications of generative models in healthcare, including Med-PaLM 2 technology, show promising results. By addressing limitations in dataset size and diversity, these models contribute to more accurate diagnoses and improved patient outcomes. However, ethical considerations and collaboration among stakeholders are essential for responsible implementation. Through experiments leveraging GANs to augment brain tumor MRI datasets, the study demonstrates how generative AI can enhance image quality and diversity, ultimately advancing medical diagnostics and patient care.","Tue, 26 Mar 2024 09:55:49 UTC (503 KB)"
"129","Determination of nuclear matter radii by means of microscopic optical potentials: the case of $^{78}$Kr","Matteo Vorabbi, Paolo Finelli, Carlotta Giusti","Nuclear Theory (nucl-th)","In this work we use microscopic Nucleon-Nucleus Optical Potentials (OP) to analyze elastic scattering data for the differential cross section of the $^{78}$Kr (p,p) $^{78}$Kr reaction, with the goal of extracting the matter radius and estimating the neutron skin, quantities that are both needed to determine the slope parameter $L$ of the nuclear symmetry energy. Our analysis is performed with the factorized version of the microscopic OP obtained in a previous series of papers within the Watson multiple scattering theory at the first order of the spectator expansion, which is based on the underlying nucleon-nucleon dynamics and is free from phenomenological inputs. Differently from our previous applications, the proton and neutron densities are described with a two-parameter Fermi (2pF) distribution, which makes the extraction of the matter radius easier and allows us to make a meaningful comparison with the original analysis, that was performed with the Glauber model. With standard minimization techniques we performed data analysis and extracted the matter radius and the neutron skin. Our analysis produces a matter radius of $R_m^{\rm (rms)} = 4.12$ fm, in good agreement with previous matter radii extracted from $^{76}$Kr and $^{80}$Kr, and a neutron skin of $\Delta R_{np} \simeq - 0.1$ fm, compatible with a previous analysis. Our factorized microscopic OP, supplied with 2pF densities, is a valuable tool to perform the analysis of the experimental differential cross section and extract information such as matter radius and neutron skin. Without any free parameters it provides a reasonably good description of the experimental differential cross section for scattering angles up to $\approx$ 40 degrees. Compared to the Glauber model our OP can be applied to a wider range of scattering angles and allows one to probe the nuclear systems in a more internal region.","Tue, 26 Mar 2024 09:46:56 UTC (105 KB)"
"130","Prediction-sharing During Training and Inference","Yotam Gafni, Ronen Gradwohl, Moshe Tennenholtz","Theoretical Economics (econ.TH)","Two firms are engaged in a competitive prediction task. Each firm has two sources of data -- labeled historical data and unlabeled inference-time data -- and uses the former to derive a prediction model, and the latter to make predictions on new instances. We study data-sharing contracts between the firms. The novelty of our study is to introduce and highlight the differences between contracts that share prediction models only, contracts to share inference-time predictions only, and contracts to share both. Our analysis proceeds on three levels. First, we develop a general Bayesian framework that facilitates our study. Second, we narrow our focus to two natural settings within this framework: (i) a setting in which the accuracy of each firm's prediction model is common knowledge, but the correlation between the respective models is unknown; and (ii) a setting in which two hypotheses exist regarding the optimal predictor, and one of the firms has a structural advantage in deducing it. Within these two settings we study optimal contract choice. More specifically, we find the individually rational and Pareto-optimal contracts for some notable cases, and describe specific settings where each of the different sharing contracts emerge as optimal. Finally, in the third level of our analysis we demonstrate the applicability of our concepts in a synthetic simulation using real loan data.","Tue, 26 Mar 2024 09:18:50 UTC (1,011 KB)"
"131","baskexact: An R package for analytical calculation of basket trial operating characteristics","Lukas Baumann","Computation (stat.CO)","Basket trials are a new type of clinical trial in which a treatment is investigated in several subgroups. For the analysis of these trials, information is shared between the subgroups based on the observed data to increase the power. Many approaches for the analysis of basket trials have been suggested, but only a few have been implemented in open source software packages. The R package baskexact facilitates the evaluation of two basket trial designs which use empirical Bayes techniques for sharing information. With baskexact, operating characteristics for single-stage and two-stage designs can be calculated analytically and optimal tuning parameters can be selected.","Tue, 26 Mar 2024 09:11:58 UTC (240 KB)"
"132","Algorithmic unfolding for image reconstruction and localization problems in fluorescence microscopy","Silvia Bonettini, Luca Calatroni, Danilo Pezzi, Marco Prato","Numerical Analysis (math.NA)","We propose an unfolded accelerated projected-gradient descent procedure to estimate model and algorithmic parameters for image super-resolution and molecule localization problems in image microscopy. The variational lower-level constraint enforces sparsity of the solution and encodes different noise statistics (Gaussian, Poisson), while the upper-level cost assesses optimality w.r.t.~the task considered. In more detail, a standard $\ell_2$ cost is considered for image reconstruction (e.g., deconvolution/super-resolution, semi-blind deconvolution) problems, while a smoothed $\ell_1$ is employed to assess localization precision in some exemplary fluorescence microscopy problems exploiting single-molecule activation. Several numerical experiments are reported to validate the proposed approach on synthetic and realistic ISBI data.","Tue, 26 Mar 2024 09:08:25 UTC (3,044 KB)"
"133","A Type of Nonlinear Fréchet Regressions","Lu Lin, Ze Chen","Methodology (stat.ME)","The existing Fréchet regression is actually defined within a linear framework, since the weight function in the Fréchet objective function is linearly defined, and the resulting Fréchet regression function is identified to be a linear model when the random object belongs to a Hilbert space. Even for nonparametric and semiparametric Fréchet regressions, which are usually nonlinear, the existing methods handle them by local linear (or local polynomial) technique, and the resulting Fréchet regressions are (locally) linear as well. We in this paper introduce a type of nonlinear Fréchet regressions. Such a framework can be utilized to fit the essentially nonlinear models in a general metric space and uniquely identify the nonlinear structure in a Hilbert space. Particularly, its generalized linear form can return to the standard linear Fréchet regression through a special choice of the weight function. Moreover, the generalized linear form possesses methodological and computational simplicity because the Euclidean variable and the metric space element are completely separable. The favorable theoretical properties (e.g. the estimation consistency and presentation theorem) of the nonlinear Fréchet regressions are established systemically. The comprehensive simulation studies and a human mortality data analysis demonstrate that the new strategy is significantly better than the competitors.","Tue, 26 Mar 2024 08:23:37 UTC (66 KB)"
"134","Investigations on Physics-Informed Neural Networks for Aerodynamics","Guillaume Coulaud (ACUMES), Maxime Le (ACUMES), Régis Duvigneau (ACUMES)","Analysis of PDEs (math.AP)","Physics-Informed Neural Networks (PINNs) have recently emerged as a novel approach to simulate complex physical systems on the basis of both data observations and physical models. In this work, we investigate the use of PINNs for various applications in aerodynamics and we explain how to leverage their specific formulation to perform some tasks effectively. In particular, we demonstrate the ability of PINNs to construct parametric surrogate models, to achieve multiphysic couplings and to infer turbulence characteristics via data assimilation. The robustness and accuracy of the PINNs approach are analysed, then current issues and challenges are discussed.","Tue, 26 Mar 2024 07:57:56 UTC (1,234 KB)"
"135","Green HPC: An analysis of the domain based on Top500","Abdessalam Benhari (LIG, DATAMOVE ), Denis Trystram (UGA, DATAMOVE ), Fanny Dufossé (DATAMOVE), Yves Denneulin, Frédéric Desprez","Computers and Society (cs.CY)","The demand in computing power has never stopped growing over the years. Today, the performance of the most powerful systems exceeds the exascale and the number of petascale systems continues to grow. Unfortunately, this growth also goes hand in hand with ever-increasing energy costs, which in turn means a significant carbon footprint. In view of the environmental crisis, this paper intents to look at the often hidden issue of energy consumption of HPC systems. As it is not easy to access the data of the constructors, we then consider the Top500 as the tip of the iceberg to identify the trends of the whole domain.The objective of this work is to analyze Top500 and Green500 data from several perspectives in order to identify the dynamic of the domain regarding its environmental impact. The contributions are to take stock of the empirical laws governing the evolution of HPC computing systems both from the performance and energy perspectives, to analyze the most relevant data for developing the performance and energy efficiency of large-scale computing systems, to put these analyses into perspective with effects and impacts (lifespan of the HPC systems) and finally to derive a predictive model for the weight of HPC sector within the horizon 2030.","Tue, 26 Mar 2024 07:55:40 UTC (1,105 KB)"
"136","Localized Inverse Design in Conservation Laws and Hamilton-Jacobi Equations","Rinaldo M. Colombo, Vincent Perrollaz (IDP)","Analysis of PDEs (math.AP)","Consider the inverse design problem for a scalar conservation law, i.e., the problem of finding initial data evolving into a given profile at a given time. The solution we present below takes into account localizations both in the final interval where the profile is assigned and in the initial interval where the datum is sought, as well as additional a priori constraints on the datum's range provided by the model. These results are motivated and can be applied to data assimilation procedures in traffic modeling and accidents localization.","Tue, 26 Mar 2024 07:53:28 UTC (21 KB)"
"137","Rapid neutron star equation of state inference with Normalising Flows","Jordan McGinn, Arunava Mukherjee, Jessica Irwin, Christopher Messenger, Michael J. Williams, Ik Siong Heng","General Relativity and Quantum Cosmology (gr-qc)","The first direct detection of gravitational waves from binary neutron stars on the 17th of August, 2017, (GW170817) heralded the arrival of a new messenger for probing neutron star astrophysics and provided the first constraints on neutron star equation of state from gravitational wave observations. Significant computational effort was expended to obtain these first results and therefore, as observations of binary neutron star coalescence become more routine in the coming observing runs, there is a need to improve the analysis speed and flexibility. Here, we present a rapid approach for inferring the neutron star equation of state based on Normalising Flows. As a demonstration, using the same input data, our approach, ASTREOS, produces results consistent with those presented by the LIGO-Virgo collaboration but requires < 1 sec to generate neutron star equation of state confidence intervals. Furthermore, ASTREOS allows for non-parametric equation of state inference. This rapid analysis will not only facilitate neutron star equation of state studies but can potentially enhance future alerts for electromagnetic follow-up observations of binary neutron star mergers.","Tue, 26 Mar 2024 07:53:21 UTC (2,333 KB)"
"138","Study on high-frequency quasi-periodic oscillations in rotating black bounce spacetime","Jianbo Lu, Shining Yang, Mou Xu, Liu Yang","General Relativity and Quantum Cosmology (gr-qc)","We investigate dynamical effects of particles moving around rotating SV regular BH and traversable wormholes, and the deviation from Kerr BH. We found that the rotating SV traversable wormhole spacetime has a closer innermost stable circular orbit to the central object than Kerr and rotating regular BHs. Furthermore, the paper provides the general form of the epicycle frequency in this spacetime, presenting the radial profile of the angular frequency of particles undergoing oscillatory motion. Unlike the commonly used $\chi$-square analysis method, with considering a generalized data analysis approach in this paper we hypothesize that the observational data of three microguasars can be explained through an axisymmetric SV spacetime, fitting the data to find the range of spin values for three types of SV objects, and analyzing the possible mechanisms of HFQPOs (high-frequency quasi-periodic oscillations) generation for different types of axisymmetric SV objects. We found that for a Kerr BH with $l^*=0$, the observational data of three microquasars can be explained by the HFQPOs model used in this paper (excluding ER4 model); for a regular BH with $l^*=1$, the HFQPO phenomenon observed through the three microquasars can be explained by the ER0, ER1, ER2, ER3, RP0, RP1, RP2, TD and WD models. For a traversable wormhole with $l^*=3$, the mechanism of HFQPO can be interpreted by the ER1, ER3, RP0, RP1, RP2, TD and WD models. Moreover, we observed that for the same HFQPOs model, the limiting range of the spin parameter $\alpha^*$ increases with the increase of the parameter $l^*$.","Tue, 26 Mar 2024 07:40:47 UTC (837 KB)"
"139","Cost-benefit analysis of ecosystem modelling to support fisheries management","Matthew H. Holden, Eva E. Plagányi, Elizabeth A. Fulton, Alexander B. Campbell, Rachel Janes, Robyn A. Lovett, Montana Wickens, Matthew P. Adams, Larissa Lubiana Botelho, Catherine M. Dichmont, Philip Erm, Kate J Helmstedt, Ryan F. Heneghan, Manuela Mendiolar, Anthony J. Richardson, Jacob G. D. Rogers, Kate Saunders, Liam Timms","Populations and Evolution (q-bio.PE)","Mathematical and statistical models underlie many of the world's most important fisheries management decisions. Since the 19th century, difficulty calibrating and fitting such models has been used to justify the selection of simple, stationary, single-species models to aid tactical fisheries management decisions. Whereas these justifications are reasonable, it is imperative that we quantify the value of different levels of model complexity for supporting fisheries management, especially given a changing climate, where old methodologies may no longer perform as well as in the past. Here we argue that cost-benefit analysis is an ideal lens to assess the value of model complexity in fisheries management. While some studies have reported the benefits of model complexity in fisheries, modeling costs are rarely considered. In the absence of cost data in the literature, we report, as a starting point, relative costs of single-species stock assessment and marine ecosystem models from two Australian organizations. We found that costs varied by two orders of magnitude, and that ecosystem model costs increased with model complexity. Using these costs, we walk through a hypothetical example of cost-benefit analysis. The demonstration is intended to catalyze the reporting of modeling costs and benefits.","Tue, 26 Mar 2024 07:24:28 UTC (474 KB)"
"140","Particle identification with machine learning from incomplete data in the ALICE experiment","Maja Karwowska, Łukasz Graczykowski, Kamil Deja, Miłosz Kasak, Małgorzata Janik (for the ALICE collaboration)","High Energy Physics - Experiment (hep-ex)","The ALICE experiment at the LHC measures properties of the strongly interacting matter formed in ultrarelativistic heavy-ion collisions. Such studies require accurate particle identification (PID). ALICE provides PID information via several detectors for particles with momentum from about 100 MeV/c up to 20 GeV/c. Traditionally, particles are selected with rectangular cuts. Acmuch better performance can be achieved with machine learning (ML) methods. Our solution uses multiple neural networks (NN) serving as binary classifiers. Moreover, we extended our particle classifier with Feature Set Embedding and attention in order to train on data with incomplete samples. We also present the integration of the ML project with the ALICE analysis software, and we discuss domain adaptation, the ML technique needed to transfer the knowledge between simulated and real experimental data.","Tue, 26 Mar 2024 07:05:06 UTC (1,608 KB)"
"141","Coupling-Constant Averaged Exchange-Correlation Hole for He, Li, Be, N, Ne Atoms from CCSD","Lin Hou, Tom J. P. Irons, Yanyong Wang, James W. Furness, Andrew M. Wibowo-Teale, Jianwei Sun","Chemical Physics (physics.chem-ph)","Accurate approximation of the exchange-correlation (XC) energy in density functional theory (DFT) calculations is essential for reliably modelling electronic systems. Many such approximations are developed from models of the XC hole; accurate reference XC holes for real electronic systems are crucial for evaluating the accuracy of these models however the availability of reliable reference data is limited to a few systems. In this study, we employ the Lieb optimization with a coupled cluster singles and doubles (CCSD) reference to construct accurate coupling-constant averaged XC holes, resolved into individual exchange and correlation components, for five spherically symmetric atoms: He, Li, Be, N, and Ne. Alongside providing a new set of reference data for the construction and evaluation of model XC holes, we compare our data against the exchange and correlation hole models of the established LDA and PBE density functional approximations. Our analysis confirms the established rationalization for the limitations of LDA and the improvement observed with PBE in terms of the hole depth and its long-range decay, demonstrated in real-space for the series of spherically-symmetric atoms.","Tue, 26 Mar 2024 06:42:09 UTC (535 KB)"
"142","Neural Clustering based Visual Representation Learning","Guikun Chen, Xia Li, Yi Yang, Wenguan Wang","Computer Vision and Pattern Recognition (cs.CV)","We investigate a fundamental aspect of machine vision: the measurement of features, by revisiting clustering, one of the most classic approaches in machine learning and data analysis. Existing visual feature extractors, including ConvNets, ViTs, and MLPs, represent an image as rectangular regions. Though prevalent, such a grid-style paradigm is built upon engineering practice and lacks explicit modeling of data distribution. In this work, we propose feature extraction with clustering (FEC), a conceptually elegant yet surprisingly ad-hoc interpretable neural clustering framework, which views feature extraction as a process of selecting representatives from data and thus automatically captures the underlying data distribution. Given an image, FEC alternates between grouping pixels into individual clusters to abstract representatives and updating the deep features of pixels with current representatives. Such an iterative working mechanism is implemented in the form of several neural layers and the final representatives can be used for downstream tasks. The cluster assignments across layers, which can be viewed and inspected by humans, make the forward process of FEC fully transparent and empower it with promising ad-hoc interpretability. Extensive experiments on various visual recognition models and tasks verify the effectiveness, generality, and interpretability of FEC. We expect this work will provoke a rethink of the current de facto grid-style paradigm.","Tue, 26 Mar 2024 06:04:50 UTC (1,157 KB)"
"143","Generalization Error Analysis for Sparse Mixture-of-Experts: A Preliminary Study","Jinze Zhao, Peihao Wang, Zhangyang Wang","Machine Learning (cs.LG)","Mixture-of-Experts (MoE) represents an ensemble methodology that amalgamates predictions from several specialized sub-models (referred to as experts). This fusion is accomplished through a router mechanism, dynamically assigning weights to each expert's contribution based on the input data. Conventional MoE mechanisms select all available experts, incurring substantial computational costs. In contrast, Sparse Mixture-of-Experts (Sparse MoE) selectively engages only a limited number, or even just one expert, significantly reducing computation overhead while empirically preserving, and sometimes even enhancing, performance. Despite its wide-ranging applications and these advantageous characteristics, MoE's theoretical underpinnings have remained elusive. In this paper, we embark on an exploration of Sparse MoE's generalization error concerning various critical factors. Specifically, we investigate the impact of the number of data samples, the total number of experts, the sparsity in expert selection, the complexity of the routing mechanism, and the complexity of individual experts. Our analysis sheds light on \textit{how \textbf{sparsity} contributes to the MoE's generalization}, offering insights from the perspective of classical learning theory.","Tue, 26 Mar 2024 05:48:02 UTC (85 KB)"
"144","Handling multivariable missing data in causal mediation analysis","S. Ghazaleh Dashti, Katherine J. Lee, Julie A. Simpson, John B. Carlin, Margarita Moreno-Betancur","Applications (stat.AP)","Mediation analysis is commonly used in epidemiological research, but guidance is lacking on how multivariable missing data should be dealt with in these analyses. Multiple imputation (MI) is a widely used approach, but questions remain regarding impact of missingness mechanism, how to ensure imputation model compatibility and approaches to variance estimation. To address these gaps, we conducted a simulation study based on the Victorian Adolescent Health Cohort Study. We considered six missingness mechanisms, involving varying assumptions regarding the influence of outcome and/or mediator on missingness in key variables. We compared the performance of complete-case analysis, seven MI approaches, differing in how the imputation model was tailored, and a ""substantive model compatible"" MI approach. We evaluated both the MI-Boot (MI, then bootstrap) and Boot-MI (bootstrap, then MI) approaches to variance estimation. Results showed that when the mediator and/or outcome influenced their own missingness, there was large bias in effect estimates, while for other mechanisms appropriate MI approaches yielded approximately unbiased estimates. Beyond incorporating all analysis variables in the imputation model, how MI was tailored for compatibility with mediation analysis did not greatly impact point estimation bias. BootMI returned variance estimates with smaller bias than MIBoot, especially in the presence of incompatibility.","Tue, 26 Mar 2024 05:25:29 UTC (2,062 KB)"
"145","Explainable Graph Neural Networks for Observation Impact Analysis in Atmospheric State Estimation","Hyeon-Ju Jeon, Jeon-Ho Kang, In-Hyuk Kwon, O-Joun Lee","Artificial Intelligence (cs.AI)","This paper investigates the impact of observations on atmospheric state estimation in weather forecasting systems using graph neural networks (GNNs) and explainability methods. We integrate observation and Numerical Weather Prediction (NWP) points into a meteorological graph, extracting $k$-hop subgraphs centered on NWP points. Self-supervised GNNs are employed to estimate the atmospheric state by aggregating data within these $k$-hop radii. The study applies gradient-based explainability methods to quantify the significance of different observations in the estimation process. Evaluated with data from 11 satellite and land-based observations, the results highlight the effectiveness of visualizing the importance of observation types, enhancing the understanding and optimization of observational data in weather forecasting.","Tue, 26 Mar 2024 05:10:47 UTC (1,554 KB)"
"146","Exploring and Applying Audio-Based Sentiment Analysis in Music","Etash Jhanji","Sound (cs.SD)","Sentiment analysis is a continuously explored area of text processing that deals with the computational analysis of opinions, sentiments, and subjectivity of text. However, this idea is not limited to text and speech, in fact, it could be applied to other modalities. In reality, humans do not express themselves in text as deeply as they do in music. The ability of a computational model to interpret musical emotions is largely unexplored and could have implications and uses in therapy and musical queuing. In this paper, two individual tasks are addressed. This study seeks to (1) predict the emotion of a musical clip over time and (2) determine the next emotion value after the music in a time series to ensure seamless transitions. Utilizing data from the Emotions in Music Database, which contains clips of songs selected from the Free Music Archive annotated with levels of valence and arousal as reported on Russel's circumplex model of affect by multiple volunteers, models are trained for both tasks. Overall, the performance of these models reflected that they were able to perform the tasks they were designed for effectively and accurately.","Thu, 22 Feb 2024 22:34:06 UTC (4,723 KB)"
"147","On the Heating of the Slow Solar-Wind by Imbalanced Alfvén-Wave Turbulence from 0.06 au to 1 au: Parker Solar Probe and Solar Orbiter observations","Sofiane Bourouaine, Jean C. Perez, Benjamin D. G. Chandran, Vamsee K. Jagarlamudi, Nour E. Raouafi, Jasper S. Halekas","Space Physics (physics.space-ph)","In this work we analyze plasma and magnetic field data provided by the Parker Solar Probe (\emph{PSP}) and Solar Orbiter (\emph{SO}) missions to investigate the radial evolution of the heating of Alfvénic slow wind (ASW) by imbalanced Alfvén-Wave (AW) turbulent fluctuations from 0.06 au to 1 au. in our analysis we focus on slow solar-wind intervals with highly imbalanced and incompressible turbulence (i.e., magnetic compressibility $C_B=\delta B/B\leq 0.25$, plasma compressibility $C_n=\delta n/n\leq 0.25$ and normalized cross-helicity $\sigma_c\geq 0.65$). First, we estimate the AW turbulent dissipation rate from the wave energy equation and find that the radial profile trend is similar to the proton heating rate. Second, we find that the scaling of the empirical AW turbulent dissipation rate $Q_W$ obtained from the wave energy equation matches the scaling from the phenomenological AW turbulent dissipation rate $Q_{\rm CH09}$ (with $Q_{\rm CH09}\simeq 1.55 Q_W$) derived by~\cite{chandran09} based on the model of reflection-driven turbulence. Our results suggest that, as in the fast solar wind, AW turbulence plays a major role in the ion heating that occurs in incompressible slow-wind streams.","Tue, 26 Mar 2024 03:31:24 UTC (543 KB)"
"148","Measurement Uncertainty Impact on Koopman Operator Estimation of Power System Dynamics","P. Algikar, P. Sharma, M. Netto, L. Mili","Applications (stat.AP)","Sensor measurements are mission-critical for monitoring and controlling power systems because they provide real-time insight into the grid operating condition; however, confidence in these insights depends greatly on the quality of the sensor data. Uncertainty in sensor measurements is an intrinsic aspect of the measurement process. In this paper, we develop an analytical method to quantify the impact of measurement uncertainties in numerical methods that employ the Koopman operator to identify nonlinear dynamics based on recorded data. In particular, we quantify the confidence interval of each element in the push-forward matrix from which a subset of the Koopman operator's discrete spectrum is estimated. We provide a detailed numerical analysis of the developed method applied to numerical simulations and field data collected from experiments conducted in a megawatt-scale facility at the National Renewable Energy Laboratory.","Tue, 26 Mar 2024 02:53:26 UTC (4,822 KB)"
"149","Labeling subtypes in a Parkinson's Cohort using Multifeatures in MRI - Integrating Grey and White Matter Information","Tanmayee Samantaray, Jitender Saini, Pramod Kumar Pal, Bithiah Grace Jaganathan, Vijaya V Saradhi, Gupta CN","Image and Video Processing (eess.IV)","Thresholding of networks has long posed a challenge in brain connectivity analysis. Weighted networks are typically binarized using threshold measures to facilitate network analysis. Previous studies on MRI-based brain networks have predominantly utilized density or sparsity-based thresholding techniques, optimized within specific ranges derived from network metrics such as path length, clustering coefficient, and small-world index. Thus, determination of a single threshold value for facilitating comparative analysis of networks remains elusive. To address this, our study introduces Mutual K-Nearest Neighbor (MKNN)-based thresholding for brain network analysis. Here, nearest neighbor selection is based on the highest correlation between features of brain regions. Construction of brain networks was accomplished by computing Pearson correlations between grey matter volume and white matter volume for each pair of brain regions. Structural MRI data from 180 Parkinsons patients and 70 controls from the NIMHANS, India were analyzed. Subtypes within Parkinsons disease were identified based on grey and white matter volume atrophy using source-based morphometric decomposition. The loading coefficients were correlated with clinical features to discern clinical relationship with the deciphered subtypes. Our data-mining approach revealed: Subtype A (N = 51, intermediate type), Subtype B (N = 57, mild-severe type with mild motor symptoms), and Subtype AB (N = 36, most-severe type with predominance in motor impairment). Subtype-specific weighted matrices were binarized using MKNN-based thresholding for brain network analysis. Permutation tests on network metrics of resulting bipartite graphs demonstrated significant group differences in betweenness centrality and participation coefficient. The identified hubs were specific to each subtype, with some hubs conserved across different subtypes.","Tue, 26 Mar 2024 02:32:52 UTC (2,741 KB)"
"150","FedMIL: Federated-Multiple Instance Learning for Video Analysis with Optimized DPP Scheduling","Ashish Bastola, Hao Wang, Xiwen Chen, Abolfazl Razi","Distributed, Parallel, and Cluster Computing (cs.DC)","Many AI platforms, including traffic monitoring systems, use Federated Learning (FL) for decentralized sensor data processing for learning-based applications while preserving privacy and ensuring secured information transfer. On the other hand, applying supervised learning to large data samples, like high-resolution images requires intensive human labor to label different parts of a data sample. Multiple Instance Learning (MIL) alleviates this challenge by operating over labels assigned to the 'bag' of instances. In this paper, we introduce Federated Multiple-Instance Learning (FedMIL). This framework applies federated learning to boost the training performance in video-based MIL tasks such as vehicle accident detection using distributed CCTV networks. However, data sources in decentralized settings are not typically Independently and Identically Distributed (IID), making client selection imperative to collectively represent the entire dataset with minimal clients. To address this challenge, we propose DPPQ, a framework based on the Determinantal Point Process (DPP) with a quality-based kernel to select clients with the most diverse datasets that achieve better performance compared to both random selection and current DPP-based client selection methods even with less data utilization in the majority of non-IID cases. This offers a significant advantage for deployment on edge devices with limited computational resources, providing a reliable solution for training AI models in massive smart sensor networks.","Tue, 26 Mar 2024 02:30:50 UTC (4,187 KB)"
